START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/cmake
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/cmake --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/cmake/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (860.7705082077159 docs/s, 0.0008208947259976539 MB/s).
Processed 2000 documents (1284.7637572174658 docs/s, 0.001225246197907892 MB/s).
Processed 3000 documents (1500.9920000610755 docs/s, 0.0014314575195894961 MB/s).
Processed 4000 documents (1426.954228929434 docs/s, 0.0013608495988172856 MB/s).
Processed 5000 documents (1536.7789323099043 docs/s, 0.0014655865977381748 MB/s).
Processed 6000 documents (1543.2076008319618 docs/s, 0.0014717174537963503 MB/s).
Processed 7000 documents (1566.434569310266 docs/s, 0.0014938684170820865 MB/s).
Processed 8000 documents (1400.7421408185673 docs/s, 0.0013358518036065744 MB/s).
Processed 9000 documents (1441.1984514252708 docs/s, 0.0013744339479687413 MB/s).
Processed 10000 documents (1460.8239522656559 docs/s, 0.0013931502840668257 MB/s).
Processed 11000 documents (1341.8463367783795 docs/s, 0.0012796843879493518 MB/s).
Processed 12000 documents (1368.7683239797095 docs/s, 0.001305359195689878 MB/s).
Processed 13000 documents (1402.0422944740878 docs/s, 0.001337091726755226 MB/s).
Processed 14000 documents (1427.2362199084341 docs/s, 0.0013611185263714162 MB/s).
Processed 15000 documents (1441.8306140261318 docs/s, 0.001375036825204975 MB/s).
Processed 16000 documents (1298.7277709815917 docs/s, 0.0012385633191886822 MB/s).
Processed 17000 documents (1295.4926037108855 docs/s, 0.0012354780232533317 MB/s).
Processed 18000 documents (1317.9618571860035 docs/s, 0.0012569063732013736 MB/s).
Processed 19000 documents (1333.0931845160085 docs/s, 0.0012713367314491353 MB/s).
Processed 20000 documents (1340.177627765164 docs/s, 0.0012780929830218926 MB/s).
Processed 21000 documents (1198.079001322261 docs/s, 0.00114257717258669 MB/s).
Processed 22000 documents (1219.190212642158 docs/s, 0.0011627103926107006 MB/s).
Processed 23000 documents (1269.4549858428038 docs/s, 0.0012106466158321417 MB/s).
Processed 24000 documents (1318.511786606444 docs/s, 0.001257430826765484 MB/s).
Processed 25000 documents (1368.1894287658822 docs/s, 0.0013048071181925604 MB/s).
Processed 26000 documents (1417.586330052663 docs/s, 0.001351915674259818 MB/s).
Processed 27000 documents (1464.2232132727067 docs/s, 0.0013963920719840114 MB/s).
Processed 28000 documents (1511.781935674886 docs/s, 0.0014417476040600643 MB/s).
Processed 29000 documents (1559.1590390899908 docs/s, 0.0014869299307727726 MB/s).
Processed 30000 documents (1605.3194045969917 docs/s, 0.0015309518857927243 MB/s).
Processed 31000 documents (1650.1314448265375 docs/s, 0.0015736879776254058 MB/s).
Processed 32000 documents (1696.4813329922617 docs/s, 0.001617890675537359 MB/s).
Processed 33000 documents (1741.2588259775296 docs/s, 0.00166059382055047 MB/s).
Processed 34000 documents (1786.4742945663002 docs/s, 0.001703714651647854 MB/s).
Processed 35000 documents (1831.3233316735893 docs/s, 0.0017464860264526265 MB/s).
Processed 36000 documents (1876.6439625329283 docs/s, 0.0017897071481065066 MB/s).
Processed 37000 documents (1920.9715719462422 docs/s, 0.001831981250711672 MB/s).
Processed 38000 documents (1964.6029287980064 docs/s, 0.0018735913551311554 MB/s).
Processed 39000 documents (2004.7162308919226 docs/s, 0.0019118463810843683 MB/s).
Processed 40000 documents (2047.6984194161346 docs/s, 0.0019528373903428408 MB/s).
Processed 41000 documents (2089.3459233522444 docs/s, 0.001992555545189137 MB/s).
Processed 42000 documents (2131.2307604362977 docs/s, 0.0020325000385630586 MB/s).
Processed 43000 documents (2172.328248104298 docs/s, 0.002071693657020853 MB/s).
Processed 44000 documents (2213.5181853757645 docs/s, 0.002110975442291035 MB/s).
Processed 45000 documents (2253.93824859826 docs/s, 0.002149523018453846 MB/s).
Processed 46000 documents (2293.574137913695 docs/s, 0.002187322748101897 MB/s).
Processed 47000 documents (2333.530745399291 docs/s, 0.002225428338431636 MB/s).
Processed 48000 documents (2374.596517871786 docs/s, 0.002264591710922037 MB/s).
Processed 49000 documents (2415.0934030833837 docs/s, 0.00230321255024279 MB/s).
Processed 50000 documents (2454.294477583471 docs/s, 0.002340597608169051 MB/s).
Processed 51000 documents (2492.1215526222395 docs/s, 0.0023766723180982966 MB/s).
Processed 52000 documents (2529.0956943123697 docs/s, 0.0024119336073993394 MB/s).
Processed 53000 documents (2567.1427511633888 docs/s, 0.0024482181083330047 MB/s).
Processed 54000 documents (2605.3434703644566 docs/s, 0.00248464915310331 MB/s).
Processed 55000 documents (2643.8142090880497 docs/s, 0.0025213377085571763 MB/s).
Processed 56000 documents (2681.754912760511 docs/s, 0.002557520783195983 MB/s).
Processed 57000 documents (2716.574189831933 docs/s, 0.002590727033454831 MB/s).
Processed 58000 documents (2752.174175068568 docs/s, 0.0026246778250394514 MB/s).
Processed 59000 documents (2786.598568112446 docs/s, 0.002657507484543272 MB/s).
Processed 60000 documents (2822.8388485823857 docs/s, 0.0026920689092468125 MB/s).
Processed 61000 documents (2857.447605977473 docs/s, 0.002725074392297242 MB/s).
Processed 62000 documents (2893.5270398010607 docs/s, 0.0027594824216852767 MB/s).
Processed 63000 documents (2930.158766030626 docs/s, 0.002794417158156038 MB/s).
Processed 64000 documents (2964.632813949454 docs/s, 0.002827294172238783 MB/s).
Processed 65000 documents (2999.2360263756545 docs/s, 0.0028602943671948 MB/s).
Processed 66000 documents (3034.028582799553 docs/s, 0.0028934751346583873 MB/s).
Processed 67000 documents (3068.572488883458 docs/s, 0.002926418770678957 MB/s).
Processed 68000 documents (3101.018293498906 docs/s, 0.0029573615012158452 MB/s).
Processed 69000 documents (3135.6127518417893 docs/s, 0.002990353347627439 MB/s).
Processed 70000 documents (3169.9654767863717 docs/s, 0.0030231146591056554 MB/s).
Processed 71000 documents (3199.9276534842593 docs/s, 0.0030516888174860566 MB/s).
Processed 72000 documents (3229.0793852657757 docs/s, 0.0030794900753648526 MB/s).
Processed 73000 documents (3262.0568575395173 docs/s, 0.003110939843692319 MB/s).
Processed 74000 documents (3295.0060214462 docs/s, 0.0031423626150571824 MB/s).
Processed 75000 documents (3327.3285690316056 docs/s, 0.0031731877985302025 MB/s).
Processed 76000 documents (3357.213688482031 docs/s, 0.003201688469392806 MB/s).
Processed 77000 documents (3389.2561091715183 docs/s, 0.0032322465030398544 MB/s).
Processed 78000 documents (3421.7223837074766 docs/s, 0.003263208755214192 MB/s).
Processed 79000 documents (3450.2619897769787 docs/s, 0.003290426244523028 MB/s).
Processed 80000 documents (3482.331308703845 docs/s, 0.0033210099303282213 MB/s).
Processed 81000 documents (3513.9693777438774 docs/s, 0.003351182344192388 MB/s).
Processed 82000 documents (3545.021143011334 docs/s, 0.003380795615207037 MB/s).
Processed 83000 documents (3576.7161683065415 docs/s, 0.0034110223467889228 MB/s).
Processed 84000 documents (3607.5869705282257 docs/s, 0.0034404630379946 MB/s).
Processed 85000 documents (3637.8885596181963 docs/s, 0.0034693608852560008 MB/s).
Processed 86000 documents (3665.8115006962053 docs/s, 0.003495990277000623 MB/s).
Processed 87000 documents (3697.899613555453 docs/s, 0.0035265918860964327 MB/s).
Processed 88000 documents (3725.9527172679677 docs/s, 0.003553345410602539 MB/s).
Processed 89000 documents (3756.7428246638447 docs/s, 0.0035827091452253768 MB/s).
Processed 90000 documents (3787.06153801842 docs/s, 0.0036116233234581184 MB/s).
Processed 91000 documents (3816.5381090488017 docs/s, 0.003639734372185518 MB/s).
Processed 92000 documents (3846.272563807969 docs/s, 0.0036680913580016796 MB/s).
Processed 93000 documents (3872.5745074198267 docs/s, 0.0036931748460958735 MB/s).
Processed 94000 documents (3901.9079678993066 docs/s, 0.0037211494139664713 MB/s).
Processed 95000 documents (3929.443306985234 docs/s, 0.0037474091596462573 MB/s).
Processed 96000 documents (3957.2732657842826 docs/s, 0.003773949876579554 MB/s).
Processed 97000 documents (3985.070970991727 docs/s, 0.003800459834090926 MB/s).
Processed 98000 documents (4013.8073041268085 docs/s, 0.0038278649369495473 MB/s).
Processed 99000 documents (4040.1591941213555 docs/s, 0.0038529960576261096 MB/s).
Processed 100000 documents (4067.877539031385 docs/s, 0.0038794303312600946 MB/s).
Processed 101000 documents (4094.572497674151 docs/s, 0.0039048886276952274 MB/s).
Processed 102000 documents (4122.313850821651 docs/s, 0.003931344843694354 MB/s).
Processed 103000 documents (4147.893302381273 docs/s, 0.003955739309674524 MB/s).
Processed 104000 documents (4174.831022549197 docs/s, 0.003981429121541211 MB/s).
Processed 105000 documents (4199.154703545738 docs/s, 0.004004625991388071 MB/s).
Processed 106000 documents (4226.5865886169395 docs/s, 0.004030787075631084 MB/s).
Processed 107000 documents (4252.815507623949 docs/s, 0.004055800922035169 MB/s).
Processed 108000 documents (4279.256796492676 docs/s, 0.004081017300121952 MB/s).
Processed 109000 documents (4303.601987436109 docs/s, 0.00410423468345271 MB/s).
Processed 110000 documents (4327.766901873483 docs/s, 0.0041272801417097885 MB/s).
Processed 111000 documents (4354.432243707073 docs/s, 0.0041527101933546764 MB/s).
Processed 112000 documents (4380.20326829471 docs/s, 0.00417728735761138 MB/s).
Processed 113000 documents (4402.794884045992 docs/s, 0.004198832401319496 MB/s).
Processed 114000 documents (4426.061428520174 docs/s, 0.004221021107216047 MB/s).
Processed 115000 documents (4450.4973440075155 docs/s, 0.004244325012214199 MB/s).
Processed 116000 documents (4472.404882773388 docs/s, 0.004265217669270885 MB/s).
Processed 117000 documents (4495.628083175431 docs/s, 0.004287365039039069 MB/s).
Processed 118000 documents (4518.491274453006 docs/s, 0.0043091690773515755 MB/s).
Processed 119000 documents (4542.790176442923 docs/s, 0.004332342316096233 MB/s).
Processed 120000 documents (4567.44744229367 docs/s, 0.004355857317250891 MB/s).
Processed 121000 documents (4589.387366248426 docs/s, 0.0043767808592304475 MB/s).
Processed 122000 documents (4608.778474730878 docs/s, 0.0043952736613568095 MB/s).
Processed 123000 documents (4634.568530099896 docs/s, 0.004419868974780937 MB/s).
Processed 124000 documents (4658.156708596317 docs/s, 0.00444236441478378 MB/s).
Processed 125000 documents (4680.875545714372 docs/s, 0.004464030786241886 MB/s).
Processed 126000 documents (4703.717752532016 docs/s, 0.004485814812213913 MB/s).
Processed 127000 documents (4723.173631833045 docs/s, 0.004504369384606404 MB/s).
Processed 128000 documents (4746.295958009881 docs/s, 0.004526420553216821 MB/s).
Processed 129000 documents (4768.20735990642 docs/s, 0.004547316894442005 MB/s).
Processed 130000 documents (4792.065178190349 docs/s, 0.004570069482984876 MB/s).
Processed 131000 documents (4814.792524020408 docs/s, 0.004591743968983085 MB/s).
Processed 132000 documents (4836.2088323701455 docs/s, 0.004612168152208467 MB/s).
Processed 133000 documents (4857.589090402645 docs/s, 0.0046325579551722 MB/s).
Processed 134000 documents (4880.641602985549 docs/s, 0.004654542544351148 MB/s).
Processed 135000 documents (4898.783589358609 docs/s, 0.004671844090803727 MB/s).
Processed 136000 documents (4918.651744671474 docs/s, 0.004690791840239977 MB/s).
Processed 137000 documents (4938.580104493894 docs/s, 0.004709797005170721 MB/s).
Processed 138000 documents (4960.382658073528 docs/s, 0.004730589540551689 MB/s).
Processed 139000 documents (4982.37197662434 docs/s, 0.004751560188888874 MB/s).
Processed 140000 documents (5005.0040446427365 docs/s, 0.004773143810885178 MB/s).
Processed 141000 documents (5028.078053903442 docs/s, 0.004795148900893633 MB/s).
Processed 142000 documents (5049.7395958369625 docs/s, 0.004815806957089388 MB/s).
Processed 143000 documents (5068.261468664296 docs/s, 0.00483347079149656 MB/s).
Processed 144000 documents (5090.362488422409 docs/s, 0.004854547966406259 MB/s).
Processed 145000 documents (5112.280277740623 docs/s, 0.004875450399151443 MB/s).
Processed 146000 documents (5133.124739778465 docs/s, 0.004895329227236237 MB/s).
Processed 147000 documents (5152.59768187767 docs/s, 0.004913900071981115 MB/s).
Processed 148000 documents (5171.293444175847 docs/s, 0.004931729740310523 MB/s).
Processed 149000 documents (5190.377545842932 docs/s, 0.0049499297579221075 MB/s).
Processed 150000 documents (5209.177015398647 docs/s, 0.0049678583291994545 MB/s).
Processed 151000 documents (5230.207268403643 docs/s, 0.004987914341357844 MB/s).
Processed 152000 documents (5249.757354824206 docs/s, 0.005006558756660658 MB/s).
Processed 153000 documents (5267.585492389256 docs/s, 0.005023560993565803 MB/s).
Processed 154000 documents (5289.22663371778 docs/s, 0.0050441995942285345 MB/s).
Processed 155000 documents (5307.173973096023 docs/s, 0.005061315510841391 MB/s).
Processed 156000 documents (5325.876834390872 docs/s, 0.005079151949301598 MB/s).
Processed 157000 documents (5344.490031695675 docs/s, 0.00509690287751739 MB/s).
Processed 158000 documents (5364.06622328362 docs/s, 0.005115572188647862 MB/s).
Processed 159000 documents (5381.447752787505 docs/s, 0.005132148506915574 MB/s).
Processed 160000 documents (5392.1318072407275 docs/s, 0.005142337615242698 MB/s).
Processed 161000 documents (5410.402658777316 docs/s, 0.005159762057092014 MB/s).
Processed 162000 documents (5429.70530863882 docs/s, 0.005178170498503513 MB/s).
Processed 163000 documents (5448.464782358206 docs/s, 0.00519606092677899 MB/s).
Processed 164000 documents (5468.406982084891 docs/s, 0.005215079290470973 MB/s).
Processed 165000 documents (5482.367983479037 docs/s, 0.005228393538931882 MB/s).
Processed 166000 documents (5500.485287818938 docs/s, 0.005245671546763361 MB/s).
Processed 167000 documents (5520.922021846306 docs/s, 0.0052651615351164876 MB/s).
Processed 168000 documents (5539.464974321794 docs/s, 0.005282845472642702 MB/s).
Processed 169000 documents (5558.929525516819 docs/s, 0.005301408315197772 MB/s).
Processed 170000 documents (5578.86937725809 docs/s, 0.005320424439676371 MB/s).
Processed 171000 documents (5595.9786438601195 docs/s, 0.005336741107807273 MB/s).
Processed 172000 documents (5614.976013464356 docs/s, 0.00535485841127811 MB/s).
Processed 173000 documents (5631.418935678258 docs/s, 0.005370539603880175 MB/s).
Processed 174000 documents (5649.981983450539 docs/s, 0.005388242705774821 MB/s).
Processed 175000 documents (5665.7543401071425 docs/s, 0.005403284397227423 MB/s).
Processed 176000 documents (5681.82993288029 docs/s, 0.0054186152771761795 MB/s).
Processed 177000 documents (5700.441167876335 docs/s, 0.0054363643339885095 MB/s).
Processed 178000 documents (5716.08777121871 docs/s, 0.005451286097735129 MB/s).
Processed 179000 documents (5728.225594333786 docs/s, 0.005462861627897058 MB/s).
Processed 180000 documents (5745.710901991519 docs/s, 0.0054795369167247 MB/s).
Processed 181000 documents (5760.906348351361 docs/s, 0.00549402842364441 MB/s).
Processed 182000 documents (5777.791596837608 docs/s, 0.005510131451451882 MB/s).
Processed 183000 documents (5793.385888694672 docs/s, 0.005525003327078506 MB/s).
Processed 184000 documents (5810.234826253146 docs/s, 0.005541071726086756 MB/s).
Processed 185000 documents (5828.612731317311 docs/s, 0.005558598262135802 MB/s).
Processed 186000 documents (5843.768960035394 docs/s, 0.005573052368197817 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/cmake
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/cmake/gpt2-preprocessed
Time to startup: 0.8963875770568848
END TIME: Wed Mar 29 01:07:09 UTC 2023
