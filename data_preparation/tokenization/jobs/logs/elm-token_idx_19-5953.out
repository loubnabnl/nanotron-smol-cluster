START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/elm
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/elm --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/elm/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (586.190935131787 docs/s, 0.0005590352393453474 MB/s).
Processed 2000 documents (917.6532620384035 docs/s, 0.0008751423473724398 MB/s).
Processed 3000 documents (1100.264404575551 docs/s, 0.0010492939038997183 MB/s).
Processed 4000 documents (1223.2944878959424 docs/s, 0.0011666245345076966 MB/s).
Processed 5000 documents (1327.1226554046395 docs/s, 0.001265642791180267 MB/s).
Processed 6000 documents (1446.844645248931 docs/s, 0.0013798185780038177 MB/s).
Processed 7000 documents (1360.831905918351 docs/s, 0.0012977904376204976 MB/s).
Processed 8000 documents (1410.0851772261863 docs/s, 0.001344762017465769 MB/s).
Processed 9000 documents (1445.757461717933 docs/s, 0.001378781758993085 MB/s).
Processed 10000 documents (1469.4678004221978 docs/s, 0.0014013937000486353 MB/s).
Processed 11000 documents (1472.3819309252917 docs/s, 0.001404172831464092 MB/s).
Processed 12000 documents (1320.8331447146688 docs/s, 0.0012596446463724793 MB/s).
Processed 13000 documents (1388.5993615330337 docs/s, 0.0013242715468721712 MB/s).
Processed 14000 documents (1469.1831422377754 docs/s, 0.0014011222288491968 MB/s).
Processed 15000 documents (1554.4457991978989 docs/s, 0.0014824350349406231 MB/s).
Processed 16000 documents (1638.1044533225086 docs/s, 0.0015622181447243772 MB/s).
Processed 17000 documents (1724.363231592684 docs/s, 0.0016444809261252252 MB/s).
Processed 18000 documents (1807.4107519164284 docs/s, 0.0017236812132992061 MB/s).
Processed 19000 documents (1888.7226572960556 docs/s, 0.0018012262890778118 MB/s).
Processed 20000 documents (1966.9490226650762 docs/s, 0.001875828764596058 MB/s).
Processed 21000 documents (2045.1767544663164 docs/s, 0.0019504325432456173 MB/s).
Processed 22000 documents (2118.1281115240745 docs/s, 0.002020004378818583 MB/s).
Processed 23000 documents (2193.2450257469695 docs/s, 0.0020916414506406493 MB/s).
Processed 24000 documents (2264.9053273122036 docs/s, 0.0021599820397493396 MB/s).
Processed 25000 documents (2335.1812776620145 docs/s, 0.002227002408658995 MB/s).
Processed 26000 documents (2406.3618108464493 docs/s, 0.0022948854549850934 MB/s).
Processed 27000 documents (2466.116987622487 docs/s, 0.002351872432348716 MB/s).
Processed 28000 documents (2532.8980264531406 docs/s, 0.0024155597939044386 MB/s).
Processed 29000 documents (2599.130331192763 docs/s, 0.0024787238418510086 MB/s).
Processed 30000 documents (2650.842426255536 docs/s, 0.0025280403387599334 MB/s).
Processed 31000 documents (2715.1391793767452 docs/s, 0.002589358500839944 MB/s).
Processed 32000 documents (2770.6748232926907 docs/s, 0.0026423214180876643 MB/s).
Processed 33000 documents (2835.368441511741 docs/s, 0.002704018060218564 MB/s).
Processed 34000 documents (2894.3160067033114 docs/s, 0.0027602348391564478 MB/s).
Processed 35000 documents (2950.963739748888 docs/s, 0.002814258327244652 MB/s).
Processed 36000 documents (3005.705039093887 docs/s, 0.0028664636984766837 MB/s).
Processed 37000 documents (3062.0475373197114 docs/s, 0.002920196091956817 MB/s).
Processed 38000 documents (3114.494740279243 docs/s, 0.0029702136423866684 MB/s).
Processed 39000 documents (3165.218073767537 docs/s, 0.0030185871827769634 MB/s).
Processed 40000 documents (3215.1014496419684 docs/s, 0.0030661596771640477 MB/s).
Processed 41000 documents (3267.976028825977 docs/s, 0.0031165848053226254 MB/s).
Processed 42000 documents (3321.4344318310136 docs/s, 0.0031675667112646233 MB/s).
Processed 43000 documents (3370.851471826371 docs/s, 0.003214694473101016 MB/s).
Processed 44000 documents (3421.5197765531457 docs/s, 0.0032630155339747864 MB/s).
Processed 45000 documents (3470.3778576484788 docs/s, 0.0033096102310642994 MB/s).
Processed 46000 documents (3517.1542450311426 docs/s, 0.0033542196703254153 MB/s).
Processed 47000 documents (3562.907836200679 docs/s, 0.003397853695107154 MB/s).
Processed 48000 documents (3604.560762342862 docs/s, 0.00343757702097212 MB/s).
Processed 49000 documents (3648.4550638940186 docs/s, 0.0034794378889980494 MB/s).
Processed 50000 documents (3693.912957215857 docs/s, 0.003522789914337022 MB/s).
Processed 51000 documents (3729.690072166558 docs/s, 0.0035569096299806193 MB/s).
Processed 52000 documents (3774.945846655896 docs/s, 0.003600068899780174 MB/s).
Processed 53000 documents (3817.64164532513 docs/s, 0.003640786786389475 MB/s).
Processed 54000 documents (3862.1093160179994 docs/s, 0.0036831944618396753 MB/s).
Processed 55000 documents (3903.8111885709595 docs/s, 0.00372296446663948 MB/s).
Processed 56000 documents (3945.424839757709 docs/s, 0.0037626503369881715 MB/s).
Processed 57000 documents (3984.201734801464 docs/s, 0.0037996308658613817 MB/s).
Processed 58000 documents (4023.370414265381 docs/s, 0.003836985029473668 MB/s).
Processed 59000 documents (4058.2728937003276 docs/s, 0.003870270627689674 MB/s).
Processed 60000 documents (4094.7762324026794 docs/s, 0.003905082924273185 MB/s).
Processed 61000 documents (4130.543108306994 docs/s, 0.00393919287520122 MB/s).
Processed 62000 documents (4167.676579889903 docs/s, 0.003974606113328841 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/elm
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/elm/gpt2-preprocessed
Time to startup: 0.8705458641052246
END TIME: Wed Mar 29 01:06:49 UTC 2023
