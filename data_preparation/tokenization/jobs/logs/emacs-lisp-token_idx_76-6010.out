START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/emacs-lisp
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/emacs-lisp --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/emacs-lisp/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (442.4510390727472 docs/s, 0.0004219541922309372 MB/s).
Processed 2000 documents (705.3747628845382 docs/s, 0.0006726977948041327 MB/s).
Processed 3000 documents (933.5519723435798 docs/s, 0.0008903045390544699 MB/s).
Processed 4000 documents (1044.2717078213059 docs/s, 0.0009958951070988711 MB/s).
Processed 5000 documents (1142.6995319630541 docs/s, 0.0010897631950026075 MB/s).
Processed 6000 documents (1215.0766349548405 docs/s, 0.0011587873792217641 MB/s).
Processed 7000 documents (1233.798864851161 docs/s, 0.0011766422890197381 MB/s).
Processed 8000 documents (1265.6659507200065 docs/s, 0.0012070331103515687 MB/s).
Processed 9000 documents (1320.2789110016877 docs/s, 0.0012591160879151227 MB/s).
Processed 10000 documents (1355.5346534090293 docs/s, 0.001292738583954839 MB/s).
Processed 11000 documents (1394.2025936501627 docs/s, 0.0013296152054311397 MB/s).
Processed 12000 documents (1427.653482828597 docs/s, 0.0013615164593015641 MB/s).
Processed 13000 documents (1461.041620215822 docs/s, 0.001393357868400404 MB/s).
Processed 14000 documents (1379.7846915529676 docs/s, 0.001315865222504585 MB/s).
Processed 15000 documents (1423.5338087514715 docs/s, 0.001357587631942245 MB/s).
Processed 16000 documents (1439.5380769951187 docs/s, 0.0013728504915190875 MB/s).
Processed 17000 documents (1465.9987793480996 docs/s, 0.0013980853837471958 MB/s).
Processed 18000 documents (1480.4367556242205 docs/s, 0.001411854510902615 MB/s).
Processed 19000 documents (1500.6422168682286 docs/s, 0.0014311239403421675 MB/s).
Processed 20000 documents (1541.4015285596518 docs/s, 0.0014699950490566747 MB/s).
Processed 21000 documents (1577.542060127338 docs/s, 0.0015044613457940463 MB/s).
Processed 22000 documents (1603.0139508982181 docs/s, 0.0015287532338125402 MB/s).
Processed 23000 documents (1631.8401853716746 docs/s, 0.00155624407326858 MB/s).
Processed 24000 documents (1678.2313627764433 docs/s, 0.001600486147667354 MB/s).
Processed 25000 documents (1726.6760997660256 docs/s, 0.0016466866490993744 MB/s).
Processed 26000 documents (1772.730605913944 docs/s, 0.0016906076487674178 MB/s).
Processed 27000 documents (1815.6777649417486 docs/s, 0.00173156525129485 MB/s).
Processed 28000 documents (1855.0053929023766 docs/s, 0.0017690710000060812 MB/s).
Processed 29000 documents (1895.215483217965 docs/s, 0.0018074183304004334 MB/s).
Processed 30000 documents (1935.6871410510148 docs/s, 0.001846015111018195 MB/s).
Processed 31000 documents (1973.4417733443574 docs/s, 0.0018820207341617178 MB/s).
Processed 32000 documents (2013.591870183986 docs/s, 0.0019203108503188953 MB/s).
Processed 33000 documents (2050.7272329689276 docs/s, 0.0019557258920373227 MB/s).
Processed 34000 documents (2088.2415653688754 docs/s, 0.0019915023473442797 MB/s).
Processed 35000 documents (2124.8662664505255 docs/s, 0.0020264303841119055 MB/s).
Processed 36000 documents (2158.409705953002 docs/s, 0.002058419900849344 MB/s).
Processed 37000 documents (2189.537400242519 docs/s, 0.0020881055834222023 MB/s).
Processed 38000 documents (2222.382958023959 docs/s, 0.002119429548286399 MB/s).
Processed 39000 documents (2257.545047536382 docs/s, 0.002152962729965574 MB/s).
Processed 40000 documents (2287.7160909021973 docs/s, 0.0021817360791227315 MB/s).
Processed 41000 documents (2315.7885636314604 docs/s, 0.0022085080753626446 MB/s).
Processed 42000 documents (2345.693688813211 docs/s, 0.002237027825177394 MB/s).
Processed 43000 documents (2374.875678063243 docs/s, 0.0022648579388267927 MB/s).
Processed 44000 documents (2404.451534933177 docs/s, 0.002293063673909356 MB/s).
Processed 45000 documents (2430.4893157971164 docs/s, 0.0023178952367755093 MB/s).
Processed 46000 documents (2458.2987446975417 docs/s, 0.0023444163748717706 MB/s).
Processed 47000 documents (2489.133171485787 docs/s, 0.002373822375760829 MB/s).
Processed 48000 documents (2513.7077866179184 docs/s, 0.002397258555047911 MB/s).
Processed 49000 documents (2537.103288774931 docs/s, 0.002419570244574481 MB/s).
Processed 50000 documents (2559.538052122795 docs/s, 0.002440965702173991 MB/s).
Processed 51000 documents (2582.866066450074 docs/s, 0.002463213030290674 MB/s).
Processed 52000 documents (2603.7317760909514 docs/s, 0.0024831121216687692 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/emacs-lisp
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/emacs-lisp/gpt2-preprocessed
Time to startup: 0.8385593891143799
END TIME: Wed Mar 29 01:06:57 UTC 2023
