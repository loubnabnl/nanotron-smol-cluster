START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/glsl
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/glsl --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/glsl/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (1030.8867564248173 docs/s, 0.000983130222725694 MB/s).
Processed 2000 documents (1443.7731726182137 docs/s, 0.0013768893934423578 MB/s).
Processed 3000 documents (1543.3202210092204 docs/s, 0.001471824856766911 MB/s).
Processed 4000 documents (1700.317286451381 docs/s, 0.0016215489258302508 MB/s).
Processed 5000 documents (1515.0258796356197 docs/s, 0.0014448412700992772 MB/s).
Processed 6000 documents (1632.6717244854008 docs/s, 0.001557037090764428 MB/s).
Processed 7000 documents (1684.316420636687 docs/s, 0.0016062893110625144 MB/s).
Processed 8000 documents (1735.1905918447196 docs/s, 0.0016548067015120693 MB/s).
Processed 9000 documents (1770.544728787554 docs/s, 0.00168852303389316 MB/s).
Processed 10000 documents (1793.8106385618496 docs/s, 0.0017107111344927307 MB/s).
Processed 11000 documents (1675.551912596089 docs/s, 0.00159793082484826 MB/s).
Processed 12000 documents (1720.5379908192665 docs/s, 0.0016408328922455468 MB/s).
Processed 13000 documents (1742.36327759892 docs/s, 0.0016616471076955032 MB/s).
Processed 14000 documents (1756.336212491827 docs/s, 0.0016749727368276853 MB/s).
Processed 15000 documents (1767.3932914264967 docs/s, 0.0016855175890221564 MB/s).
Processed 16000 documents (1794.7132111527735 docs/s, 0.001711571894791387 MB/s).
Processed 17000 documents (1809.856727141624 docs/s, 0.0017260138770500413 MB/s).
Processed 18000 documents (1717.9141484728916 docs/s, 0.00163833060118951 MB/s).
Processed 19000 documents (1725.5794377009981 docs/s, 0.0016456407906541807 MB/s).
Processed 20000 documents (1725.7122423764504 docs/s, 0.0016457674430622581 MB/s).
Processed 21000 documents (1739.8470991583488 docs/s, 0.0016592474929412353 MB/s).
Processed 22000 documents (1768.421460941652 docs/s, 0.0016864981278816719 MB/s).
Processed 23000 documents (1773.0845960237166 docs/s, 0.0016909452400433699 MB/s).
Processed 24000 documents (1787.2742745703504 docs/s, 0.0017044775720313553 MB/s).
Processed 25000 documents (1704.8086361345565 docs/s, 0.0016258322106690946 MB/s).
Processed 26000 documents (1743.4577629910555 docs/s, 0.0016626908903036646 MB/s).
Processed 27000 documents (1785.3055963802974 docs/s, 0.0017026000942042326 MB/s).
Processed 28000 documents (1795.5134100982298 docs/s, 0.001712335023973684 MB/s).
Processed 29000 documents (1790.9923029694191 docs/s, 0.0017080233602232162 MB/s).
Processed 30000 documents (1792.47113268657 docs/s, 0.0017094336821428012 MB/s).
Processed 31000 documents (1783.0977291335782 docs/s, 0.001700494507917002 MB/s).
Processed 32000 documents (1805.7498756221967 docs/s, 0.0017220972782346694 MB/s).
Processed 33000 documents (1840.8842594497082 docs/s, 0.0017556040377137263 MB/s).
Processed 34000 documents (1889.6967300038984 docs/s, 0.0018021552372015938 MB/s).
Processed 35000 documents (1938.2478511223 docs/s, 0.0018484571944449425 MB/s).
Processed 36000 documents (1983.2776773270186 docs/s, 0.0018914009831686198 MB/s).
Processed 37000 documents (2029.928636393965 docs/s, 0.0019358908046664858 MB/s).
Processed 38000 documents (2076.9095648923217 docs/s, 0.001980695309536287 MB/s).
Processed 39000 documents (2117.6880627901314 docs/s, 0.0020195847156430544 MB/s).
Processed 40000 documents (2160.53432621423 docs/s, 0.002060446096624594 MB/s).
Processed 41000 documents (2192.5282349116706 docs/s, 0.0020909578656307893 MB/s).
Processed 42000 documents (2228.410780284829 docs/s, 0.0021251781275604524 MB/s).
Processed 43000 documents (2269.2113059836493 docs/s, 0.0021640885410152905 MB/s).
Processed 44000 documents (2311.9556120059087 docs/s, 0.0022048526878413284 MB/s).
Processed 45000 documents (2353.7541740629504 docs/s, 0.0022447149029378418 MB/s).
Processed 46000 documents (2390.0562216616845 docs/s, 0.0022793352333657117 MB/s).
Processed 47000 documents (2434.16514185827 docs/s, 0.0023214007776816083 MB/s).
Processed 48000 documents (2475.5881456607453 docs/s, 0.0023609048325164274 MB/s).
Processed 49000 documents (2517.3374676672915 docs/s, 0.0024007200886414447 MB/s).
Processed 50000 documents (2558.522384612568 docs/s, 0.0024399970861554796 MB/s).
Processed 51000 documents (2602.4190277602265 docs/s, 0.0024818601873018518 MB/s).
Processed 52000 documents (2641.1929807301753 docs/s, 0.002518837910394836 MB/s).
Processed 53000 documents (2675.821335468873 docs/s, 0.0025518620829285362 MB/s).
Processed 54000 documents (2716.6885142515953 docs/s, 0.002590836061717601 MB/s).
Processed 55000 documents (2755.8624852255107 docs/s, 0.002628195271707068 MB/s).
Processed 56000 documents (2785.239499788914 docs/s, 0.0026562113759888784 MB/s).
Processed 57000 documents (2825.4436956882837 docs/s, 0.0026945530850298726 MB/s).
Processed 58000 documents (2866.079733052079 docs/s, 0.002733306630184249 MB/s).
Processed 59000 documents (2903.1151128658676 docs/s, 0.0027686263207110096 MB/s).
Processed 60000 documents (2938.026322476293 docs/s, 0.0028019202446711473 MB/s).
Processed 61000 documents (2975.518881256032 docs/s, 0.002837675935035736 MB/s).
Processed 62000 documents (3012.970051908061 docs/s, 0.0028733921546059238 MB/s).
Processed 63000 documents (3050.2034706786962 docs/s, 0.002908900709799477 MB/s).
Processed 64000 documents (3086.8037973297087 docs/s, 0.0029438055012986266 MB/s).
Processed 65000 documents (3120.534294362137 docs/s, 0.002975973409998071 MB/s).
Processed 66000 documents (3156.802781613076 docs/s, 0.003010561734784199 MB/s).
Processed 67000 documents (3187.237438704697 docs/s, 0.003039586485581109 MB/s).
Processed 68000 documents (3206.402483057271 docs/s, 0.0030578636961529457 MB/s).
Processed 69000 documents (3240.618741258015 docs/s, 0.0030904948628025196 MB/s).
Processed 70000 documents (3273.3782602540887 docs/s, 0.003121736774686898 MB/s).
Processed 71000 documents (3306.1103709690697 docs/s, 0.003152952547997541 MB/s).
Processed 72000 documents (3342.74500313177 docs/s, 0.0031878900557820986 MB/s).
Processed 73000 documents (3378.5573455306067 docs/s, 0.003222043366938216 MB/s).
Processed 74000 documents (3398.303410862939 docs/s, 0.003240874682295741 MB/s).
Processed 75000 documents (3428.1828032429667 docs/s, 0.003269369891398398 MB/s).
Processed 76000 documents (3462.820939701275 docs/s, 0.003302403392506862 MB/s).
Processed 77000 documents (3493.446268854262 docs/s, 0.003331609982351553 MB/s).
Processed 78000 documents (3525.5576103018498 docs/s, 0.003362233743955469 MB/s).
Processed 79000 documents (3555.541591636534 docs/s, 0.0033908286968579618 MB/s).
Processed 80000 documents (3580.1641995338077 docs/s, 0.0034143106456125333 MB/s).
Processed 81000 documents (3613.0074922087233 docs/s, 0.003445632450302814 MB/s).
Processed 82000 documents (3617.3012997541923 docs/s, 0.003449727344278519 MB/s).
Processed 83000 documents (3643.517841568437 docs/s, 0.0034747293868717544 MB/s).
Processed 84000 documents (3672.801743605822 docs/s, 0.0035026566921289655 MB/s).
Processed 85000 documents (3702.8116448241817 docs/s, 0.0035312763641588037 MB/s).
Processed 86000 documents (3734.7476063420836 docs/s, 0.003561732870428165 MB/s).
Processed 87000 documents (3765.0360929872304 docs/s, 0.0035906182222244553 MB/s).
Processed 88000 documents (3795.823730317608 docs/s, 0.0036199796012092666 MB/s).
Processed 89000 documents (3808.0084161713835 docs/s, 0.003631599823161491 MB/s).
Processed 90000 documents (3834.2079293648417 docs/s, 0.0036565856259964387 MB/s).
Processed 91000 documents (3863.4121724565907 docs/s, 0.0036844369625631244 MB/s).
Processed 92000 documents (3889.2802231592086 docs/s, 0.0037091066581337057 MB/s).
Processed 93000 documents (3918.240490157109 docs/s, 0.003736725320965871 MB/s).
Processed 94000 documents (3947.507707868396 docs/s, 0.0037646367148097956 MB/s).
Processed 95000 documents (3964.937104682465 docs/s, 0.003781258682901826 MB/s).
Processed 96000 documents (3995.7922501773087 docs/s, 0.0038106844426892363 MB/s).
Processed 97000 documents (4022.388603913972 docs/s, 0.0038360487021579476 MB/s).
Processed 98000 documents (4052.808103892865 docs/s, 0.003865058998005738 MB/s).
Processed 99000 documents (4072.9802848990375 docs/s, 0.003884296688937223 MB/s).
Processed 100000 documents (4102.451906883713 docs/s, 0.003912403017886841 MB/s).
Processed 101000 documents (4128.413297510359 docs/s, 0.003937161729345664 MB/s).
Processed 102000 documents (4157.950629999694 docs/s, 0.0039653307247158945 MB/s).
Processed 103000 documents (4179.339368190813 docs/s, 0.003985728614989102 MB/s).
Processed 104000 documents (4200.743416523422 docs/s, 0.004006141106151029 MB/s).
Processed 105000 documents (4227.350608826063 docs/s, 0.00403151570208174 MB/s).
Processed 106000 documents (4244.289315593466 docs/s, 0.0040476697116789495 MB/s).
Processed 107000 documents (4268.007057119826 docs/s, 0.004070288712615801 MB/s).
Processed 108000 documents (4295.218447285707 docs/s, 0.004096239516530711 MB/s).
Processed 109000 documents (4321.212418918394 docs/s, 0.004121029299658198 MB/s).
Processed 110000 documents (4347.990810566332 docs/s, 0.004146567164007503 MB/s).
Processed 111000 documents (4373.709464337162 docs/s, 0.004171094383561289 MB/s).
Processed 112000 documents (4399.888268651421 docs/s, 0.0041960604368700225 MB/s).
Processed 113000 documents (4426.1410785708 docs/s, 0.00422109706742363 MB/s).
Processed 114000 documents (4449.304863019803 docs/s, 0.004243187773723414 MB/s).
Processed 115000 documents (4474.6361825803115 docs/s, 0.004267345602588951 MB/s).
Processed 116000 documents (4502.5019258557095 docs/s, 0.004293920446258268 MB/s).
Processed 117000 documents (4527.381774548415 docs/s, 0.0043176477189525745 MB/s).
Processed 118000 documents (4547.242044910697 docs/s, 0.004336587948713967 MB/s).
Processed 119000 documents (4572.240773230039 docs/s, 0.004360428593854941 MB/s).
Processed 120000 documents (4597.0949046885735 docs/s, 0.00438413134068353 MB/s).
Processed 121000 documents (4622.415969542214 docs/s, 0.004408279389898504 MB/s).
Processed 122000 documents (4645.686686674727 docs/s, 0.004430472075152137 MB/s).
Processed 123000 documents (4658.42441516433 docs/s, 0.004442619719662027 MB/s).
Processed 124000 documents (4682.513296414706 docs/s, 0.004465592667021471 MB/s).
Processed 125000 documents (4703.2899355571735 docs/s, 0.004485406814152883 MB/s).
Processed 126000 documents (4725.528442870861 docs/s, 0.004506615107413159 MB/s).
Processed 127000 documents (4747.339777688188 docs/s, 0.004527416017234982 MB/s).
Processed 128000 documents (4764.756210673489 docs/s, 0.004544025622056473 MB/s).
Processed 129000 documents (4787.3704675779645 docs/s, 0.004565592258050885 MB/s).
Processed 130000 documents (4812.119767541033 docs/s, 0.004589195029774697 MB/s).
Processed 131000 documents (4832.610589910985 docs/s, 0.004608736600790963 MB/s).
Processed 132000 documents (4849.849993584318 docs/s, 0.004625177377304381 MB/s).
Processed 133000 documents (4871.873593072943 docs/s, 0.0046461807184914996 MB/s).
Processed 134000 documents (4885.944311716851 docs/s, 0.004659599601475574 MB/s).
Processed 135000 documents (4899.5149175042025 docs/s, 0.004672541539673045 MB/s).
Processed 136000 documents (4920.357952903226 docs/s, 0.0046924190072090395 MB/s).
Processed 137000 documents (4941.111573991274 docs/s, 0.004712211202613138 MB/s).
Processed 138000 documents (4961.7638106826535 docs/s, 0.00473190671032205 MB/s).
Processed 139000 documents (4981.415958798685 docs/s, 0.00475064845924252 MB/s).
Processed 140000 documents (4991.104067270542 docs/s, 0.004759887759466688 MB/s).
Processed 141000 documents (5008.592517231505 docs/s, 0.004776566045028214 MB/s).
Processed 142000 documents (5020.710214100218 docs/s, 0.004788122381305902 MB/s).
Processed 143000 documents (5038.68478841594 docs/s, 0.004805264271179142 MB/s).
Processed 144000 documents (5041.1214961227515 docs/s, 0.004807588096735717 MB/s).
Processed 145000 documents (5051.97740222813 docs/s, 0.0048179410955697345 MB/s).
Processed 146000 documents (5073.081276763977 docs/s, 0.004838067318691232 MB/s).
Processed 147000 documents (5091.847952777759 docs/s, 0.004855964615609893 MB/s).
Processed 148000 documents (5112.909368827488 docs/s, 0.004876050347163666 MB/s).
Processed 149000 documents (5130.6485269137775 docs/s, 0.00489296772662523 MB/s).
Processed 150000 documents (5145.940346916426 docs/s, 0.004907551142612864 MB/s).
Processed 151000 documents (5164.377107319204 docs/s, 0.0049251338074867285 MB/s).
Processed 152000 documents (5183.5062607297905 docs/s, 0.004943376789789 MB/s).
Processed 153000 documents (5202.480908079814 docs/s, 0.004961472423629583 MB/s).
Processed 154000 documents (5209.397611668716 docs/s, 0.004968068706196514 MB/s).
Processed 155000 documents (5228.940950660776 docs/s, 0.004986706686650063 MB/s).
Processed 156000 documents (5246.041959312672 docs/s, 0.0050030154793860165 MB/s).
Processed 157000 documents (5262.527832016905 docs/s, 0.005018737632767587 MB/s).
Processed 158000 documents (5279.017295723111 docs/s, 0.005034463210795508 MB/s).
Processed 159000 documents (5294.989229500272 docs/s, 0.005049695233822128 MB/s).
Processed 160000 documents (5313.052304342488 docs/s, 0.005066921524374474 MB/s).
Processed 161000 documents (5327.307357258815 docs/s, 0.005080516202219786 MB/s).
Processed 162000 documents (5343.331708837949 docs/s, 0.005095798214757871 MB/s).
Processed 163000 documents (5356.99265272421 docs/s, 0.005108826306080064 MB/s).
Processed 164000 documents (5370.8723501863915 docs/s, 0.005122063017069236 MB/s).
Processed 165000 documents (5387.583010513211 docs/s, 0.005137999544633113 MB/s).
Processed 166000 documents (5404.672786867247 docs/s, 0.005154297625415084 MB/s).
Processed 167000 documents (5419.116650170613 docs/s, 0.005168072366877187 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/glsl
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/glsl/gpt2-preprocessed
Time to startup: 0.9365220069885254
END TIME: Wed Mar 29 01:07:08 UTC 2023
