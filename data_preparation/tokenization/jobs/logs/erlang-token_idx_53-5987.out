START TIME: Wed Mar 29 01:06:13 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/erlang
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/erlang --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/erlang/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (424.00168941408003 docs/s, 0.0004043595213070679 MB/s).
Processed 2000 documents (685.2289702109557 docs/s, 0.0006534852697476918 MB/s).
Processed 3000 documents (879.0655055440967 docs/s, 0.000838342195076081 MB/s).
Processed 4000 documents (968.9662198594569 docs/s, 0.0009240781973452157 MB/s).
Processed 5000 documents (1069.2341381664448 docs/s, 0.0010197011357941101 MB/s).
Processed 6000 documents (1092.7330701828203 docs/s, 0.001042111463721104 MB/s).
Processed 7000 documents (1160.993462836528 docs/s, 0.0011072096470227508 MB/s).
Processed 8000 documents (1215.5225087389233 docs/s, 0.0011592125975980027 MB/s).
Processed 9000 documents (1278.2919957108904 docs/s, 0.0012190742451771645 MB/s).
Processed 10000 documents (1329.5573766557995 docs/s, 0.0012679647223051067 MB/s).
Processed 11000 documents (1354.1157156435138 docs/s, 0.001291385379451288 MB/s).
Processed 12000 documents (1318.1076156025415 docs/s, 0.001257045379259626 MB/s).
Processed 13000 documents (1369.4262843229938 docs/s, 0.001305986675570482 MB/s).
Processed 14000 documents (1394.7129115222403 docs/s, 0.0013301018824789432 MB/s).
Processed 15000 documents (1413.2130402053892 docs/s, 0.0013477449800542729 MB/s).
Processed 16000 documents (1452.8143835518279 docs/s, 0.001385511764098957 MB/s).
Processed 17000 documents (1453.835662823063 docs/s, 0.001386485731909812 MB/s).
Processed 18000 documents (1474.138690010724 docs/s, 0.0014058482074839822 MB/s).
Processed 19000 documents (1501.484946634695 docs/s, 0.0014319276300761175 MB/s).
Processed 20000 documents (1535.0712931108658 docs/s, 0.0014639580660923631 MB/s).
Processed 21000 documents (1533.2755374996862 docs/s, 0.0014622455000874387 MB/s).
Processed 22000 documents (1532.6920626613473 docs/s, 0.0014616890551198457 MB/s).
Processed 23000 documents (1551.0847802504568 docs/s, 0.0014792297174934929 MB/s).
Processed 24000 documents (1566.5673782440701 docs/s, 0.0014939950735512449 MB/s).
Processed 25000 documents (1549.3102316227485 docs/s, 0.0014775373760440335 MB/s).
Processed 26000 documents (1554.422820883358 docs/s, 0.0014824131211122113 MB/s).
Processed 27000 documents (1477.7793847218386 docs/s, 0.0014093202445238481 MB/s).
Processed 28000 documents (1477.437232747973 docs/s, 0.0014089939429740648 MB/s).
Processed 29000 documents (1497.1950644714946 docs/s, 0.001427836479636664 MB/s).
Processed 30000 documents (1497.4522307850532 docs/s, 0.0014280817325449497 MB/s).
Processed 31000 documents (1509.254660945966 docs/s, 0.0014393374070605907 MB/s).
Processed 32000 documents (1506.9156326842756 docs/s, 0.0014371067358820682 MB/s).
Processed 33000 documents (1500.039518920622 docs/s, 0.0014305491627889843 MB/s).
Processed 34000 documents (1505.7168822440335 docs/s, 0.0014359635183754287 MB/s).
Processed 35000 documents (1493.5454598155716 docs/s, 0.0014243559454112736 MB/s).
Processed 36000 documents (1520.7790903962023 docs/s, 0.0014503279594385169 MB/s).
Processed 37000 documents (1541.684708044788 docs/s, 0.0014702651100585824 MB/s).
Processed 38000 documents (1558.6795495209417 docs/s, 0.0014864726538857858 MB/s).
Processed 39000 documents (1585.2608157939712 docs/s, 0.0015118225248279297 MB/s).
Processed 40000 documents (1614.1160651595364 docs/s, 0.001539341035041367 MB/s).
Processed 41000 documents (1641.958604242898 docs/s, 0.0015658937494687061 MB/s).
Processed 42000 documents (1667.676012064811 docs/s, 0.0015904197807930097 MB/s).
Processed 43000 documents (1692.276897206035 docs/s, 0.0016138810131130553 MB/s).
Processed 44000 documents (1719.3569540393416 docs/s, 0.0016397065678018013 MB/s).
Processed 45000 documents (1747.5881226397396 docs/s, 0.0016666299082181354 MB/s).
Processed 46000 documents (1775.1654318922153 docs/s, 0.001692929679767814 MB/s).
Processed 47000 documents (1800.0186838912507 docs/s, 0.0017166315878784663 MB/s).
Processed 48000 documents (1825.3842486193491 docs/s, 0.001740822075480794 MB/s).
Processed 49000 documents (1847.4171670905316 docs/s, 0.0017618343039422337 MB/s).
Processed 50000 documents (1871.8796178554019 docs/s, 0.0017851635149530429 MB/s).
Processed 51000 documents (1895.1457064861054 docs/s, 0.0018073517861233763 MB/s).
Processed 52000 documents (1919.1663195552885 docs/s, 0.0018302596278717885 MB/s).
Processed 53000 documents (1938.5289769551857 docs/s, 0.001848725296931444 MB/s).
Processed 54000 documents (1961.5233234274965 docs/s, 0.0018706544145846334 MB/s).
Processed 55000 documents (1983.3675385772578 docs/s, 0.001891486681535013 MB/s).
Processed 56000 documents (2005.1177915772632 docs/s, 0.0019122293391964562 MB/s).
Processed 57000 documents (2028.3794220466668 docs/s, 0.0019344133587328595 MB/s).
Processed 58000 documents (2051.073761545043 docs/s, 0.0019560563674402647 MB/s).
Processed 59000 documents (2074.070617251568 docs/s, 0.001977987878085678 MB/s).
Processed 60000 documents (2096.28096032297 docs/s, 0.0019991693118314455 MB/s).
Processed 61000 documents (2117.3100761954265 docs/s, 0.0020192242395357385 MB/s).
Processed 62000 documents (2139.2923500388683 docs/s, 0.002040188169516438 MB/s).
Processed 63000 documents (2159.1803857588066 docs/s, 0.002059154878386313 MB/s).
Processed 64000 documents (2180.609513909184 docs/s, 0.002079591287526306 MB/s).
Processed 65000 documents (2198.817613680979 docs/s, 0.0020969558846292294 MB/s).
Processed 66000 documents (2217.8069126268847 docs/s, 0.0021150654913205 MB/s).
Processed 67000 documents (2238.647895341305 docs/s, 0.002134941001263909 MB/s).
Processed 68000 documents (2255.317820140496 docs/s, 0.0021508386804013216 MB/s).
Processed 69000 documents (2271.681485969789 docs/s, 0.0021664442882249725 MB/s).
Processed 70000 documents (2288.9782707444865 docs/s, 0.0021829397876210084 MB/s).
Processed 71000 documents (2307.3186171382094 docs/s, 0.002200430504930696 MB/s).
Processed 72000 documents (2324.965522763291 docs/s, 0.0022172599055893813 MB/s).
Processed 73000 documents (2341.3084030708264 docs/s, 0.0022328456907947792 MB/s).
Processed 74000 documents (2358.5364875067935 docs/s, 0.0022492756724422393 MB/s).
Processed 75000 documents (2377.772241424767 docs/s, 0.0022676203169105214 MB/s).
Processed 76000 documents (2392.6746455939046 docs/s, 0.0022818323570193335 MB/s).
Processed 77000 documents (2411.5028423031636 docs/s, 0.0022997883246451984 MB/s).
Processed 78000 documents (2428.7607828061855 docs/s, 0.0023162467792569976 MB/s).
Processed 79000 documents (2447.1685869387525 docs/s, 0.0023338018292796635 MB/s).
Processed 80000 documents (2462.9042214100223 docs/s, 0.0023488084997272704 MB/s).
Processed 81000 documents (2479.307680264256 docs/s, 0.0023644520571367796 MB/s).
Processed 82000 documents (2496.876593180898 docs/s, 0.0023812070781525593 MB/s).
Processed 83000 documents (2513.224090682911 docs/s, 0.0023967972666577445 MB/s).
Processed 84000 documents (2530.602625422697 docs/s, 0.0024133707288958523 MB/s).
Processed 85000 documents (2547.4058307350115 docs/s, 0.0024293955142355075 MB/s).
Processed 86000 documents (2565.148597005581 docs/s, 0.0024463163347297487 MB/s).
Processed 87000 documents (2581.2178757592715 docs/s, 0.002461641193160316 MB/s).
Processed 88000 documents (2596.7478054487174 docs/s, 0.0024764516882407354 MB/s).
Processed 89000 documents (2610.770894067116 docs/s, 0.002489825147692791 MB/s).
Processed 90000 documents (2625.9000547721967 docs/s, 0.002504253439686009 MB/s).
Processed 91000 documents (2640.721401426011 docs/s, 0.0025183881773243057 MB/s).
Processed 92000 documents (2652.1819600035674 docs/s, 0.0025293178176913904 MB/s).
Processed 93000 documents (2665.049559966853 docs/s, 0.002541589317290166 MB/s).
Processed 94000 documents (2676.561506297511 docs/s, 0.002552567964837562 MB/s).
Processed 95000 documents (2689.69927672159 docs/s, 0.0025650971190658476 MB/s).
Processed 96000 documents (2702.4940786047664 docs/s, 0.002577299193005339 MB/s).
Processed 97000 documents (2718.975866104919 docs/s, 0.0025930174504327002 MB/s).
Processed 98000 documents (2730.8948943570035 docs/s, 0.0026043843215532336 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/erlang
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/erlang/gpt2-preprocessed
Time to startup: 0.8505120277404785
END TIME: Wed Mar 29 01:07:12 UTC 2023
