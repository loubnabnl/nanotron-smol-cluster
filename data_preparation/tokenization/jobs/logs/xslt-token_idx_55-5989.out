START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/xslt
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/xslt --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/xslt/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (407.12974820452195 docs/s, 0.00038826918430759615 MB/s).
Processed 2000 documents (500.8366075710761 docs/s, 0.0004776350093565713 MB/s).
Processed 3000 documents (615.5241129331996 docs/s, 0.0005870095376331326 MB/s).
Processed 4000 documents (716.8390883814428 docs/s, 0.0006836310275854519 MB/s).
Processed 5000 documents (804.0846533565091 docs/s, 0.0007668348821225253 MB/s).
Processed 6000 documents (869.4378871661696 docs/s, 0.0008291605827008911 MB/s).
Processed 7000 documents (923.5843599094413 docs/s, 0.0008807986830801404 MB/s).
Processed 8000 documents (957.2500933017263 docs/s, 0.0009129048283593429 MB/s).
Processed 9000 documents (980.4398938274009 docs/s, 0.0009350203455232628 MB/s).
Processed 10000 documents (1010.9804184345691 docs/s, 0.0009641460594506922 MB/s).
Processed 11000 documents (1050.0348324843128 docs/s, 0.0010013912510722282 MB/s).
Processed 12000 documents (1077.7697974936216 docs/s, 0.001027841374868032 MB/s).
Processed 13000 documents (1103.9500495670486 docs/s, 0.0010528088088675009 MB/s).
Processed 14000 documents (1135.1127165686357 docs/s, 0.001082527844017635 MB/s).
Processed 15000 documents (1135.1964663251172 docs/s, 0.0010826077140093967 MB/s).
Processed 16000 documents (1147.873064333995 docs/s, 0.00109469705994987 MB/s).
Processed 17000 documents (1175.1847928944198 docs/s, 0.0011207435540146063 MB/s).
Processed 18000 documents (1199.146192029029 docs/s, 0.001143594924954442 MB/s).
Processed 19000 documents (1224.2347232576078 docs/s, 0.0011675212128234938 MB/s).
Processed 20000 documents (1163.7139772838632 docs/s, 0.0011098041317785866 MB/s).
Processed 21000 documents (1193.7020618836411 docs/s, 0.0011384029978596126 MB/s).
Processed 22000 documents (1230.3279922184608 docs/s, 0.0011733322069344147 MB/s).
Processed 23000 documents (1263.5422839603032 docs/s, 0.001205007823906234 MB/s).
Processed 24000 documents (1296.994354587213 docs/s, 0.0012369102044937257 MB/s).
Processed 25000 documents (1329.2480900463356 docs/s, 0.0012676697636092526 MB/s).
Processed 26000 documents (1361.0961139368703 docs/s, 0.0012980424060219482 MB/s).
Processed 27000 documents (1391.113016982699 docs/s, 0.0013266687555148116 MB/s).
Processed 28000 documents (1413.343740812373 docs/s, 0.001347869625866292 MB/s).
Processed 29000 documents (1438.4935374189001 docs/s, 0.001371854340952778 MB/s).
Processed 30000 documents (1468.4337631728754 docs/s, 0.0014004075652817491 MB/s).
Processed 31000 documents (1494.6900184546018 docs/s, 0.0014254474815889376 MB/s).
Processed 32000 documents (1518.3497975601147 docs/s, 0.0014480112052537104 MB/s).
Processed 33000 documents (1544.83034984888 docs/s, 0.0014732650278557586 MB/s).
Processed 34000 documents (1569.9538366007398 docs/s, 0.00149722465190958 MB/s).
Processed 35000 documents (1592.1044112270636 docs/s, 0.001518349086024345 MB/s).
Processed 36000 documents (1614.706753927486 docs/s, 0.0015399043597483502 MB/s).
Processed 37000 documents (1636.6703511526528 docs/s, 0.0015608504783178833 MB/s).
Processed 38000 documents (1657.4521811056327 docs/s, 0.0015806695757919623 MB/s).
Processed 39000 documents (1679.083862264171 docs/s, 0.0016012991545335494 MB/s).
Processed 40000 documents (1703.0738640942018 docs/s, 0.0016241778031293886 MB/s).
Processed 41000 documents (1722.4187703896653 docs/s, 0.0016426265434166577 MB/s).
Processed 42000 documents (1741.086331136335 docs/s, 0.0016604293166507102 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/xslt
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/xslt/gpt2-preprocessed
Time to startup: 0.8466029167175293
END TIME: Wed Mar 29 01:07:00 UTC 2023
