START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/systemverilog
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/systemverilog --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/systemverilog/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (410.67601929665324 docs/s, 0.0003916511719671757 MB/s).
Processed 2000 documents (609.9694121202347 docs/s, 0.000581712162132487 MB/s).
Processed 3000 documents (748.9373859444248 docs/s, 0.0007142423495716332 MB/s).
Processed 4000 documents (851.89267072505 docs/s, 0.0008124281604052067 MB/s).
Processed 5000 documents (949.814136159326 docs/s, 0.0009058133470147381 MB/s).
Processed 6000 documents (990.1579593522482 docs/s, 0.0009442882150194628 MB/s).
Processed 7000 documents (994.7721832552111 docs/s, 0.0009486886818458663 MB/s).
Processed 8000 documents (1055.9328159201175 docs/s, 0.0010070160063935447 MB/s).
Processed 9000 documents (1104.086332556323 docs/s, 0.001052938778454135 MB/s).
Processed 10000 documents (1146.9889415473192 docs/s, 0.0010938538947556679 MB/s).
Processed 11000 documents (1191.5522809196368 docs/s, 0.0011363528069683426 MB/s).
Processed 12000 documents (1221.6719201531862 docs/s, 0.0011650771333248007 MB/s).
Processed 13000 documents (1209.9478634191648 docs/s, 0.0011538962015334748 MB/s).
Processed 14000 documents (1158.740615465394 docs/s, 0.001105061164346117 MB/s).
Processed 15000 documents (1187.3935910305909 docs/s, 0.0011323867712312611 MB/s).
Processed 16000 documents (1205.3282408984549 docs/s, 0.0011494905861839818 MB/s).
Processed 17000 documents (1216.2945800773455 docs/s, 0.0011599489022038893 MB/s).
Processed 18000 documents (1242.7663555586355 docs/s, 0.0011851943545900683 MB/s).
Processed 19000 documents (1270.3165985935714 docs/s, 0.0012114683137832369 MB/s).
Processed 20000 documents (1305.1425751692777 docs/s, 0.0012446809531872537 MB/s).
Processed 21000 documents (1339.707115261765 docs/s, 0.001277644267331853 MB/s).
Processed 22000 documents (1355.536282301775 docs/s, 0.0012927401373880147 MB/s).
Processed 23000 documents (1395.771103590173 docs/s, 0.0013311110530759554 MB/s).
Processed 24000 documents (1431.9955747009974 docs/s, 0.0013656574007997488 MB/s).
Processed 25000 documents (1467.0267899269727 docs/s, 0.001399065771033261 MB/s).
Processed 26000 documents (1502.6144069669606 docs/s, 0.0014330047673863989 MB/s).
Processed 27000 documents (1536.9794366437695 docs/s, 0.0014657778135717101 MB/s).
Processed 28000 documents (1572.7425407445878 docs/s, 0.0014998841674276236 MB/s).
Processed 29000 documents (1598.8556311692444 docs/s, 0.0015247875510876125 MB/s).
Processed 30000 documents (1629.375170976817 docs/s, 0.0015538932523506326 MB/s).
Processed 31000 documents (1663.1373234821028 docs/s, 0.0015860913500615147 MB/s).
Processed 32000 documents (1693.2014829072946 docs/s, 0.001614762766749663 MB/s).
Processed 33000 documents (1712.466062652548 docs/s, 0.0016331349016690713 MB/s).
Processed 34000 documents (1731.7646153619908 docs/s, 0.001651539435731879 MB/s).
Processed 35000 documents (1752.067490300296 docs/s, 0.0016709017661097488 MB/s).
Processed 36000 documents (1782.4270067273526 docs/s, 0.0016998548571847464 MB/s).
Processed 37000 documents (1806.837168312093 docs/s, 0.0017231342013474397 MB/s).
Processed 38000 documents (1827.7607063416378 docs/s, 0.0017430884421745661 MB/s).
Processed 39000 documents (1853.2699018338685 docs/s, 0.0017674159067476926 MB/s).
Processed 40000 documents (1877.868484971845 docs/s, 0.0017908749437063646 MB/s).
Processed 41000 documents (1896.68263749594 docs/s, 0.001808817517753544 MB/s).
Processed 42000 documents (1924.1879374652133 docs/s, 0.0018350486158992894 MB/s).
Processed 43000 documents (1942.530293034544 docs/s, 0.0018525412493081512 MB/s).
Processed 44000 documents (1964.0566101881682 docs/s, 0.001873070345104378 MB/s).
Processed 45000 documents (1986.9891279894398 docs/s, 0.0018949404983419798 MB/s).
Processed 46000 documents (2008.4712067174955 docs/s, 0.0019154274050879436 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/systemverilog
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/systemverilog/gpt2-preprocessed
Time to startup: 0.7890405654907227
END TIME: Wed Mar 29 01:07:00 UTC 2023
