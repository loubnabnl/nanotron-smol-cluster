START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/ocaml
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/ocaml --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/ocaml/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (400.62880982896473 docs/s, 0.0003820694063462875 MB/s).
Processed 2000 documents (641.0470715908576 docs/s, 0.0006113501277836396 MB/s).
Processed 3000 documents (801.7848775037713 docs/s, 0.000764641644958278 MB/s).
Processed 4000 documents (924.6278967617773 docs/s, 0.0008817938773744366 MB/s).
Processed 5000 documents (1018.0761687147916 docs/s, 0.0009709130942485729 MB/s).
Processed 6000 documents (1080.2195926746126 docs/s, 0.001030177681612599 MB/s).
Processed 7000 documents (999.9698579882104 docs/s, 0.0009536455707437614 MB/s).
Processed 8000 documents (1059.3730640580632 docs/s, 0.0010102968826847679 MB/s).
Processed 9000 documents (1126.0256386278088 docs/s, 0.0010738617311742867 MB/s).
Processed 10000 documents (1159.7762452181016 docs/s, 0.0011060488178425804 MB/s).
Processed 11000 documents (1195.2717261108512 docs/s, 0.0011398999463184845 MB/s).
Processed 12000 documents (1219.2753933160848 docs/s, 0.0011627916272316787 MB/s).
Processed 13000 documents (1251.33917345677 docs/s, 0.001193370030838747 MB/s).
Processed 14000 documents (1276.1245760767194 docs/s, 0.001217007232739181 MB/s).
Processed 15000 documents (1303.783410499518 docs/s, 0.001243384752749937 MB/s).
Processed 16000 documents (1315.73490302452 docs/s, 0.0012547825842137528 MB/s).
Processed 17000 documents (1339.9330277482163 docs/s, 0.001277859714267937 MB/s).
Processed 18000 documents (1347.7630329746712 docs/s, 0.0012853269891497337 MB/s).
Processed 19000 documents (1240.7277518020628 docs/s, 0.0011832501905460957 MB/s).
Processed 20000 documents (1260.5531554905094 docs/s, 0.001202157168856153 MB/s).
Processed 21000 documents (1280.2536030631416 docs/s, 0.00122094497972788 MB/s).
Processed 22000 documents (1293.0246636092654 docs/s, 0.0012331244121639876 MB/s).
Processed 23000 documents (1319.7696388959262 docs/s, 0.0012586304081877958 MB/s).
Processed 24000 documents (1321.5189955177607 docs/s, 0.0012602987246682746 MB/s).
Processed 25000 documents (1342.8720153743027 docs/s, 0.0012806625512831714 MB/s).
Processed 26000 documents (1358.1361402036312 docs/s, 0.0012952195550953209 MB/s).
Processed 27000 documents (1369.0581009753387 docs/s, 0.001305635548568095 MB/s).
Processed 28000 documents (1373.2662866432277 docs/s, 0.0013096487871582295 MB/s).
Processed 29000 documents (1389.5807436570772 docs/s, 0.0013252074657984516 MB/s).
Processed 30000 documents (1410.9857180625836 docs/s, 0.0013456208401323162 MB/s).
Processed 31000 documents (1284.4716153671316 docs/s, 0.0012249675897284809 MB/s).
Processed 32000 documents (1304.138025573509 docs/s, 0.0012437229400382127 MB/s).
Processed 33000 documents (1322.3955758019054 docs/s, 0.0012611346967715314 MB/s).
Processed 34000 documents (1324.0125874393239 docs/s, 0.0012626767992394675 MB/s).
Processed 35000 documents (1348.7799819137479 docs/s, 0.0012862968272340277 MB/s).
Processed 36000 documents (1354.1790377903937 docs/s, 0.0012914457681564271 MB/s).
Processed 37000 documents (1362.3579696088848 docs/s, 0.00129924580536736 MB/s).
Processed 38000 documents (1367.2659363740875 docs/s, 0.0013039264072171092 MB/s).
Processed 39000 documents (1366.4536364424177 docs/s, 0.0013031517376350572 MB/s).
Processed 40000 documents (1380.1751810266849 docs/s, 0.001316237622286496 MB/s).
Processed 41000 documents (1391.8199818615346 docs/s, 0.0013273429697623583 MB/s).
Processed 42000 documents (1399.5346567553731 docs/s, 0.0013347002570680362 MB/s).
Processed 43000 documents (1407.696476237136 docs/s, 0.0013424839746829377 MB/s).
Processed 44000 documents (1330.5221862675212 docs/s, 0.0012688848364520275 MB/s).
Processed 45000 documents (1337.5788337782997 docs/s, 0.001275614579942989 MB/s).
Processed 46000 documents (1338.9110449268267 docs/s, 0.0012768850754993693 MB/s).
Processed 47000 documents (1342.377016560476 docs/s, 0.0012801904836277733 MB/s).
Processed 48000 documents (1348.6117584403805 docs/s, 0.0012861363968280607 MB/s).
Processed 49000 documents (1363.6027496167885 docs/s, 0.0013004329200904737 MB/s).
Processed 50000 documents (1373.4624066509482 docs/s, 0.001309835821772526 MB/s).
Processed 51000 documents (1386.3048485792365 docs/s, 0.0013220833287994733 MB/s).
Processed 52000 documents (1399.667390671184 docs/s, 0.001334826841994461 MB/s).
Processed 53000 documents (1408.6835234349833 docs/s, 0.0013434252962446053 MB/s).
Processed 54000 documents (1430.0186613266403 docs/s, 0.0013637720692888644 MB/s).
Processed 55000 documents (1451.1495656500836 docs/s, 0.00138392407002457 MB/s).
Processed 56000 documents (1470.621873427731 docs/s, 0.0014024943098332702 MB/s).
Processed 57000 documents (1489.7283751120121 docs/s, 0.0014207156897659418 MB/s).
Processed 58000 documents (1508.9533944335396 docs/s, 0.0014390500969252964 MB/s).
Processed 59000 documents (1527.7193903282875 docs/s, 0.0014569467452319026 MB/s).
Processed 60000 documents (1547.0186269513074 docs/s, 0.0014753519315255236 MB/s).
Processed 61000 documents (1566.4809784231682 docs/s, 0.0014939126762611085 MB/s).
Processed 62000 documents (1584.3928376777021 docs/s, 0.0015109947563912412 MB/s).
Processed 63000 documents (1601.2946529602566 docs/s, 0.001527113583526856 MB/s).
Processed 64000 documents (1619.7620494032046 docs/s, 0.0015447254652053877 MB/s).
Processed 65000 documents (1635.7642568773379 docs/s, 0.0015599863594792727 MB/s).
Processed 66000 documents (1652.8512079772468 docs/s, 0.0015762817458889453 MB/s).
Processed 67000 documents (1669.6569773734618 docs/s, 0.0015923089765295618 MB/s).
Processed 68000 documents (1686.8072498900829 docs/s, 0.0016086647509480313 MB/s).
Processed 69000 documents (1703.37372697881 docs/s, 0.001624463774660883 MB/s).
Processed 70000 documents (1719.4184101953056 docs/s, 0.0016397651769593292 MB/s).
Processed 71000 documents (1734.886278819223 docs/s, 0.0016545164859955053 MB/s).
Processed 72000 documents (1751.961249960623 docs/s, 0.0016708004474264365 MB/s).
Processed 73000 documents (1768.8231754419623 docs/s, 0.0016868812326831459 MB/s).
Processed 74000 documents (1785.388261732607 docs/s, 0.0017026789300275868 MB/s).
Processed 75000 documents (1801.57718755452 docs/s, 0.0017181178927941512 MB/s).
Processed 76000 documents (1818.462293128425 docs/s, 0.0017342207843097925 MB/s).
Processed 77000 documents (1832.674318339884 docs/s, 0.001747774427738079 MB/s).
Processed 78000 documents (1848.5008177107277 docs/s, 0.0017628677537066724 MB/s).
Processed 79000 documents (1863.2398335431603 docs/s, 0.0017769239745551685 MB/s).
Processed 80000 documents (1877.6188028640408 docs/s, 0.0017906368282928856 MB/s).
Processed 81000 documents (1893.7908470500222 docs/s, 0.001806059691476843 MB/s).
Processed 82000 documents (1907.4939053390374 docs/s, 0.0018191279462232946 MB/s).
Processed 83000 documents (1922.3516177982294 docs/s, 0.0018332973649961752 MB/s).
Processed 84000 documents (1938.2960832065862 docs/s, 0.001848503192144953 MB/s).
Processed 85000 documents (1952.7437616073203 docs/s, 0.0018622815719674304 MB/s).
Processed 86000 documents (1966.3872158486156 docs/s, 0.0018752929838644176 MB/s).
Processed 87000 documents (1980.9492118445291 docs/s, 0.001889180385441331 MB/s).
Processed 88000 documents (1994.4240987462324 docs/s, 0.0019020310389959644 MB/s).
Processed 89000 documents (2007.8093457211826 docs/s, 0.0019147962052547289 MB/s).
Processed 90000 documents (2020.8454592582093 docs/s, 0.001927228411920747 MB/s).
Processed 91000 documents (2033.4363500969703 docs/s, 0.0019392360211343482 MB/s).
Processed 92000 documents (2043.8596048909244 docs/s, 0.0019491764115247005 MB/s).
Processed 93000 documents (2057.361109709017 docs/s, 0.0019620524499025507 MB/s).
Processed 94000 documents (2071.0958297935144 docs/s, 0.001975150899690165 MB/s).
Processed 95000 documents (2083.4368557983084 docs/s, 0.0019869202192290387 MB/s).
Processed 96000 documents (2096.4891600105766 docs/s, 0.0019993678665262 MB/s).
Processed 97000 documents (2109.869685429219 docs/s, 0.00201212852995798 MB/s).
Processed 98000 documents (2122.1293213589192 docs/s, 0.0020238202298726265 MB/s).
Processed 99000 documents (2136.015112042618 docs/s, 0.0020370627518106633 MB/s).
Processed 100000 documents (2149.624186671069 docs/s, 0.0020500413767538727 MB/s).
Processed 101000 documents (2162.62941696532 docs/s, 0.0020624441308644487 MB/s).
Processed 102000 documents (2176.4673568668222 docs/s, 0.0020756410187404845 MB/s).
Processed 103000 documents (2188.7046218505106 docs/s, 0.0020873113840584856 MB/s).
Processed 104000 documents (2201.727004104933 docs/s, 0.0020997304955529526 MB/s).
Processed 105000 documents (2213.8191629794346 docs/s, 0.002111262476901469 MB/s).
Processed 106000 documents (2225.433714861306 docs/s, 0.0021223389767277773 MB/s).
Processed 107000 documents (2236.4040160684035 docs/s, 0.002132801071232227 MB/s).
Processed 108000 documents (2249.172261996886 docs/s, 0.0021449778194397792 MB/s).
Processed 109000 documents (2258.483271537219 docs/s, 0.0021538574900982085 MB/s).
Processed 110000 documents (2268.8748341797886 docs/s, 0.0021637676564977538 MB/s).
Processed 111000 documents (2281.5795128505933 docs/s, 0.0021758837822442945 MB/s).
Processed 112000 documents (2294.4112632595497 docs/s, 0.0021881210930438516 MB/s).
Processed 113000 documents (2306.0618017934357 docs/s, 0.00219923191241592 MB/s).
Processed 114000 documents (2319.291254668249 docs/s, 0.0022118485018427364 MB/s).
Processed 115000 documents (2330.1926073228196 docs/s, 0.0022222448418834874 MB/s).
Processed 116000 documents (2338.765644791078 docs/s, 0.002230420727530554 MB/s).
Processed 117000 documents (2351.122841401726 docs/s, 0.002242205468560911 MB/s).
Processed 118000 documents (2363.2350308868026 docs/s, 0.0022537565525882746 MB/s).
Processed 119000 documents (2375.2696823764036 docs/s, 0.0022652336906208072 MB/s).
Processed 120000 documents (2384.146258192653 docs/s, 0.002273699052994397 MB/s).
Processed 121000 documents (2393.3947518981436 docs/s, 0.0022825191039067684 MB/s).
Processed 122000 documents (2404.8807813238504 docs/s, 0.0022934730351675515 MB/s).
Processed 123000 documents (2416.3959143141906 docs/s, 0.002304454721750441 MB/s).
Processed 124000 documents (2426.3435600623134 docs/s, 0.0023139415360091337 MB/s).
Processed 125000 documents (2438.300629546148 docs/s, 0.0023253446860753515 MB/s).
Processed 126000 documents (2448.630989439014 docs/s, 0.002335196484984411 MB/s).
Processed 127000 documents (2458.935590318337 docs/s, 0.0023450237181838387 MB/s).
Processed 128000 documents (2469.87712392533 docs/s, 0.002355458377766924 MB/s).
Processed 129000 documents (2480.012163703903 docs/s, 0.0023651239048995045 MB/s).
Processed 130000 documents (2490.3515463191707 docs/s, 0.0023749843085471827 MB/s).
Processed 131000 documents (2501.6936181736837 docs/s, 0.002385800951169666 MB/s).
Processed 132000 documents (2511.424265609818 docs/s, 0.0023950808197115117 MB/s).
Processed 133000 documents (2520.6833357286587 docs/s, 0.0024039109570776545 MB/s).
Processed 134000 documents (2530.483699068182 docs/s, 0.0024132573118860073 MB/s).
Processed 135000 documents (2541.098329406051 docs/s, 0.0024233802122173797 MB/s).
Processed 136000 documents (2549.790101342859 docs/s, 0.002431669331877574 MB/s).
Processed 137000 documents (2559.524734691237 docs/s, 0.002440953001681554 MB/s).
Processed 138000 documents (2569.227285745203 docs/s, 0.0024502060754253417 MB/s).
Processed 139000 documents (2578.5717661314525 docs/s, 0.0024591176663698697 MB/s).
Processed 140000 documents (2588.0560412179625 docs/s, 0.002468162575929606 MB/s).
Processed 141000 documents (2598.3194030192 docs/s, 0.002477950480479431 MB/s).
Processed 142000 documents (2607.3069617892406 docs/s, 0.0024865216844456106 MB/s).
Processed 143000 documents (2616.8856924626803 docs/s, 0.0024956566738726428 MB/s).
Processed 144000 documents (2627.310728102597 docs/s, 0.002505598762610051 MB/s).
Processed 145000 documents (2634.7852782891996 docs/s, 0.0025127270491497036 MB/s).
Processed 146000 documents (2642.266876881844 docs/s, 0.0025198620575731696 MB/s).
Processed 147000 documents (2650.9114973003543 docs/s, 0.002528106210041384 MB/s).
Processed 148000 documents (2658.1440754641835 docs/s, 0.0025350037340776286 MB/s).
Processed 149000 documents (2666.7912347253655 docs/s, 0.0025432503077748923 MB/s).
Processed 150000 documents (2677.1417831905937 docs/s, 0.0025531213600068986 MB/s).
Processed 151000 documents (2683.8859727051713 docs/s, 0.0025595531203319276 MB/s).
Processed 152000 documents (2693.0597196297194 docs/s, 0.00256830188715908 MB/s).
Processed 153000 documents (2701.5413067078243 docs/s, 0.0025763905589178317 MB/s).
Processed 154000 documents (2709.339744894913 docs/s, 0.00258382772912494 MB/s).
Processed 155000 documents (2717.1728016919537 docs/s, 0.002591297914211229 MB/s).
Processed 156000 documents (2726.7557858638847 docs/s, 0.0026004369600905273 MB/s).
Processed 157000 documents (2736.4766184880036 docs/s, 0.0026097074684982334 MB/s).
Processed 158000 documents (2743.668374860631 docs/s, 0.002616566061840659 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/ocaml
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/ocaml/gpt2-preprocessed
Time to startup: 0.815518856048584
END TIME: Wed Mar 29 01:07:34 UTC 2023
