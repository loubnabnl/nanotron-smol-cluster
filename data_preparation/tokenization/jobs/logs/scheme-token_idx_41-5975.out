START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/scheme
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/scheme --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/scheme/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (555.6885719812293 docs/s, 0.0005299459190189641 MB/s).
Processed 2000 documents (853.2885440176683 docs/s, 0.0008137593689133342 MB/s).
Processed 3000 documents (1011.0945562828359 docs/s, 0.0009642549097851142 MB/s).
Processed 4000 documents (1125.4011583896042 docs/s, 0.0010732661804100077 MB/s).
Processed 5000 documents (1203.738355522147 docs/s, 0.001147974353334567 MB/s).
Processed 6000 documents (1270.81039801448 docs/s, 0.0012119392376084136 MB/s).
Processed 7000 documents (1252.6367440529132 docs/s, 0.0011946074905900127 MB/s).
Processed 8000 documents (1297.942416025611 docs/s, 0.0012378143463379011 MB/s).
Processed 9000 documents (1355.729124223891 docs/s, 0.0012929240457762632 MB/s).
Processed 10000 documents (1440.180082942729 docs/s, 0.0013734627561023036 MB/s).
Processed 11000 documents (1517.8095464399642 docs/s, 0.0014474959816360132 MB/s).
Processed 12000 documents (1619.3050619369535 docs/s, 0.0015442896479959045 MB/s).
Processed 13000 documents (1729.405111204437 docs/s, 0.0016492892372173662 MB/s).
Processed 14000 documents (1836.1139717751864 docs/s, 0.0017510547368766655 MB/s).
Processed 15000 documents (1930.0430800040126 docs/s, 0.00184063251495744 MB/s).
Processed 16000 documents (2022.1782256503443 docs/s, 0.0019284994369986957 MB/s).
Processed 17000 documents (2107.4353547804367 docs/s, 0.002009806971340596 MB/s).
Processed 18000 documents (2192.674079973248 docs/s, 0.0020910969543201903 MB/s).
Processed 19000 documents (2285.042899584567 docs/s, 0.002179186725220267 MB/s).
Processed 20000 documents (2373.9767389215867 docs/s, 0.002264000643655383 MB/s).
Processed 21000 documents (2459.4302305714164 docs/s, 0.0023454954438890613 MB/s).
Processed 22000 documents (2534.922100502206 docs/s, 0.0024174901013395366 MB/s).
Processed 23000 documents (2614.9021761027484 docs/s, 0.002493765045264004 MB/s).
Processed 24000 documents (2686.7985436032227 docs/s, 0.0025623307643921115 MB/s).
Processed 25000 documents (2760.9967189308154 docs/s, 0.0026330916585262445 MB/s).
Processed 26000 documents (2834.7373594756455 docs/s, 0.0027034162134891944 MB/s).
Processed 27000 documents (2901.4006484239103 docs/s, 0.002766991280006323 MB/s).
Processed 28000 documents (2973.831234850942 docs/s, 0.0028360664700040265 MB/s).
Processed 29000 documents (3029.8958898447904 docs/s, 0.002889533891529837 MB/s).
Processed 30000 documents (3093.518000661289 docs/s, 0.002950208664571084 MB/s).
Processed 31000 documents (3158.5410548146456 docs/s, 0.003012219481291433 MB/s).
Processed 32000 documents (3207.3042321265393 docs/s, 0.00305872367108015 MB/s).
Processed 33000 documents (3259.441314386385 docs/s, 0.0031084454673637246 MB/s).
Processed 34000 documents (3316.7328692951096 docs/s, 0.0031630829518271538 MB/s).
Processed 35000 documents (3363.256255686639 docs/s, 0.0032074511105409996 MB/s).
Processed 36000 documents (3411.63841919988 docs/s, 0.003253591937255745 MB/s).
Processed 37000 documents (3470.215571667734 docs/s, 0.00330945546309255 MB/s).
Processed 38000 documents (3512.163785792841 docs/s, 0.0033494603975227745 MB/s).
Processed 39000 documents (3561.48384361036 docs/s, 0.003396495669947014 MB/s).
Processed 40000 documents (3609.7549094681167 docs/s, 0.003442530545681111 MB/s).
Processed 41000 documents (3658.284717409298 docs/s, 0.003488812177094744 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/scheme
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/scheme/gpt2-preprocessed
Time to startup: 0.7838821411132812
END TIME: Wed Mar 29 01:06:46 UTC 2023
