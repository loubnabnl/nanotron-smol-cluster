START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/visual-basic
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/visual-basic --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/visual-basic/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (584.4265921300415 docs/s, 0.0005573526307392516 MB/s).
Processed 2000 documents (709.9707264095723 docs/s, 0.0006770808471770976 MB/s).
Processed 3000 documents (799.5707463974902 docs/s, 0.0007625300849890616 MB/s).
Processed 4000 documents (910.1659294075779 docs/s, 0.000868001870544031 MB/s).
Processed 5000 documents (1001.0903790683116 docs/s, 0.0009547141829188457 MB/s).
Processed 6000 documents (1056.357259619011 docs/s, 0.00100742078744794 MB/s).
Processed 7000 documents (1123.6598759149647 docs/s, 0.0010716055640363356 MB/s).
Processed 8000 documents (1169.883158926968 docs/s, 0.0011156875218648605 MB/s).
Processed 9000 documents (1184.299001243289 docs/s, 0.0011294355404312983 MB/s).
Processed 10000 documents (1226.0084840419406 docs/s, 0.0011692128029269606 MB/s).
Processed 11000 documents (1256.4830865707042 docs/s, 0.0011982756486613314 MB/s).
Processed 12000 documents (1203.9198903059898 docs/s, 0.0011481474783954523 MB/s).
Processed 13000 documents (1232.6035430499587 docs/s, 0.0011755023413180911 MB/s).
Processed 14000 documents (1265.7620804036935 docs/s, 0.0012071247867619453 MB/s).
Processed 15000 documents (1285.9554188238824 docs/s, 0.001226382654975779 MB/s).
Processed 16000 documents (1307.0478847075049 docs/s, 0.0012464979979586648 MB/s).
Processed 17000 documents (1305.2812720468091 docs/s, 0.0012448132248371211 MB/s).
Processed 18000 documents (1310.173002771343 docs/s, 0.0012494783427918843 MB/s).
Processed 19000 documents (1317.151033247166 docs/s, 0.0012561331112357769 MB/s).
Processed 20000 documents (1324.6750166413265 docs/s, 0.0012633085409558549 MB/s).
Processed 21000 documents (1323.7414993600516 docs/s, 0.0012624182695007817 MB/s).
Processed 22000 documents (1328.2538565338766 docs/s, 0.00126672158864391 MB/s).
Processed 23000 documents (1327.966367781864 docs/s, 0.0012664474180048599 MB/s).
Processed 24000 documents (1328.2880799680831 docs/s, 0.001266754226654132 MB/s).
Processed 25000 documents (1153.7616709773808 docs/s, 0.0011003128728650864 MB/s).
Processed 26000 documents (1162.6551163277704 docs/s, 0.0011087943232801155 MB/s).
Processed 27000 documents (1175.10312951804 docs/s, 0.001120665673749962 MB/s).
Processed 28000 documents (1173.5364096727399 docs/s, 0.0011191715332724951 MB/s).
Processed 29000 documents (1177.6980416963154 docs/s, 0.0011231403748477129 MB/s).
Processed 30000 documents (1183.8597363938113 docs/s, 0.0011290166248262513 MB/s).
Processed 31000 documents (1184.9066911464386 docs/s, 0.0011300150786842714 MB/s).
Processed 32000 documents (1193.174570332982 docs/s, 0.0011378999427156277 MB/s).
Processed 33000 documents (1209.338655417067 docs/s, 0.001153315215508525 MB/s).
Processed 34000 documents (1212.491649504848 docs/s, 0.0011563221449898224 MB/s).
Processed 35000 documents (1216.9872608600406 docs/s, 0.0011606094940758138 MB/s).
Processed 36000 documents (1220.607970266223 docs/s, 0.0011640624716436605 MB/s).
Processed 37000 documents (1231.3703002502227 docs/s, 0.00117432622933409 MB/s).
Processed 38000 documents (1104.7291424691982 docs/s, 0.0010535518097583754 MB/s).
Processed 39000 documents (1109.4792940865477 docs/s, 0.0010580819073548772 MB/s).
Processed 40000 documents (1110.2072471572274 docs/s, 0.0010587761375019334 MB/s).
Processed 41000 documents (1116.1262037123756 docs/s, 0.0010644208943485028 MB/s).
Processed 42000 documents (1118.937003301974 docs/s, 0.001067101481725668 MB/s).
Processed 43000 documents (1131.6900689324505 docs/s, 0.0010792637528728966 MB/s).
Processed 44000 documents (1139.3444381936858 docs/s, 0.0010865635282456263 MB/s).
Processed 45000 documents (1138.2985784716727 docs/s, 0.0010855661186901786 MB/s).
Processed 46000 documents (1133.413889939086 docs/s, 0.0010809077166930065 MB/s).
Processed 47000 documents (1140.8188763384514 docs/s, 0.001087969662035419 MB/s).
Processed 48000 documents (1145.2991302263563 docs/s, 0.001092242365099293 MB/s).
Processed 49000 documents (1147.6696505165703 docs/s, 0.00109450306941659 MB/s).
Processed 50000 documents (1151.6945619419698 docs/s, 0.0010983415240688036 MB/s).
Processed 51000 documents (1044.3881774730294 docs/s, 0.0009960061812143606 MB/s).
Processed 52000 documents (1054.4923255161264 docs/s, 0.0010056422476922287 MB/s).
Processed 53000 documents (1059.9485732234357 docs/s, 0.00101084573099464 MB/s).
Processed 54000 documents (1068.6463510145777 docs/s, 0.0010191405782838609 MB/s).
Processed 55000 documents (1072.5914742462635 docs/s, 0.0010229029409849772 MB/s).
Processed 56000 documents (1073.9315969183035 docs/s, 0.0010241809815581355 MB/s).
Processed 57000 documents (1077.6671278214685 docs/s, 0.0010277434614386258 MB/s).
Processed 58000 documents (1087.1772546854463 docs/s, 0.0010368130251745666 MB/s).
Processed 59000 documents (1095.8890938197537 docs/s, 0.0010451212824056184 MB/s).
Processed 60000 documents (1109.5204659658787 docs/s, 0.0010581211719187533 MB/s).
Processed 61000 documents (1123.815488400633 docs/s, 0.0010717539676672297 MB/s).
Processed 62000 documents (1138.0201930159906 docs/s, 0.0010853006296310335 MB/s).
Processed 63000 documents (1151.5695621183238 docs/s, 0.001098222314947437 MB/s).
Processed 64000 documents (1165.578747976936 docs/s, 0.0011115825156945572 MB/s).
Processed 65000 documents (1180.0010648282196 docs/s, 0.0011253367088586994 MB/s).
Processed 66000 documents (1193.8153647388128 docs/s, 0.0011385110518825653 MB/s).
Processed 67000 documents (1207.4021528859018 docs/s, 0.001151468422780897 MB/s).
Processed 68000 documents (1221.109601422788 docs/s, 0.0011645408643939858 MB/s).
Processed 69000 documents (1234.0944969944192 docs/s, 0.0011769242258018676 MB/s).
Processed 70000 documents (1247.2723470679566 docs/s, 0.0011894916029624525 MB/s).
Processed 71000 documents (1259.4755540459294 docs/s, 0.0012011294880351347 MB/s).
Processed 72000 documents (1271.6282436590861 docs/s, 0.0012127191959944593 MB/s).
Processed 73000 documents (1285.1412116982228 docs/s, 0.0012256061665518025 MB/s).
Processed 74000 documents (1297.7686904632192 docs/s, 0.0012376486687309448 MB/s).
Processed 75000 documents (1310.0348463168978 docs/s, 0.0012493465865296342 MB/s).
Processed 76000 documents (1322.173660968872 docs/s, 0.0012609230622948379 MB/s).
Processed 77000 documents (1334.4751800702074 docs/s, 0.0012726547051145625 MB/s).
Processed 78000 documents (1346.8202930834805 docs/s, 0.0012844279223284535 MB/s).
Processed 79000 documents (1358.450624309193 docs/s, 0.0012955194705097132 MB/s).
Processed 80000 documents (1370.5892497590135 docs/s, 0.0013070957658376822 MB/s).
Processed 81000 documents (1383.1011122781026 docs/s, 0.0013190280077725435 MB/s).
Processed 82000 documents (1394.3103734657304 docs/s, 0.0013297179922730736 MB/s).
Processed 83000 documents (1405.9549979518927 docs/s, 0.0013408231715697219 MB/s).
Processed 84000 documents (1418.0142599966753 docs/s, 0.0013523237800566437 MB/s).
Processed 85000 documents (1430.0651456185478 docs/s, 0.001363816400164173 MB/s).
Processed 86000 documents (1441.523288954181 docs/s, 0.0013747437371770678 MB/s).
Processed 87000 documents (1452.6338075589476 docs/s, 0.0013853395534123875 MB/s).
Processed 88000 documents (1464.2744411706235 docs/s, 0.001396440926714538 MB/s).
Processed 89000 documents (1475.3914670149136 docs/s, 0.001407042948737062 MB/s).
Processed 90000 documents (1486.7426363109998 docs/s, 0.0014178682673559187 MB/s).
Processed 91000 documents (1498.467106790124 docs/s, 0.0014290495937253228 MB/s).
Processed 92000 documents (1509.6382950146592 docs/s, 0.0014397032690188019 MB/s).
Processed 93000 documents (1520.2235442207514 docs/s, 0.0014497981493194116 MB/s).
Processed 94000 documents (1531.0059630798 docs/s, 0.0014600810652540207 MB/s).
Processed 95000 documents (1542.368282148763 docs/s, 0.0014709170171249037 MB/s).
Processed 96000 documents (1552.678757655038 docs/s, 0.0014807498528051739 MB/s).
Processed 97000 documents (1563.4501022635643 docs/s, 0.0014910222075114863 MB/s).
Processed 98000 documents (1574.304942840195 docs/s, 0.0015013741901781035 MB/s).
Processed 99000 documents (1584.650304834341 docs/s, 0.0015112402962058457 MB/s).
Processed 100000 documents (1594.6483556413057 docs/s, 0.0015207751804745728 MB/s).
Processed 101000 documents (1604.863765941527 docs/s, 0.0015305173549094457 MB/s).
Processed 102000 documents (1615.0607333441455 docs/s, 0.0015402419408265548 MB/s).
Processed 103000 documents (1624.4434384569538 docs/s, 0.0015491899857110537 MB/s).
Processed 104000 documents (1635.5673305681755 docs/s, 0.001559798555916 MB/s).
Processed 105000 documents (1645.3409393083448 docs/s, 0.001569119395550103 MB/s).
Processed 106000 documents (1656.1263375121762 docs/s, 0.001579405152809311 MB/s).
Processed 107000 documents (1666.0724216279773 docs/s, 0.0015888904777793668 MB/s).
Processed 108000 documents (1675.440283278282 docs/s, 0.0015978243668349095 MB/s).
Processed 109000 documents (1685.1108140113863 docs/s, 0.0016070469036210883 MB/s).
Processed 110000 documents (1695.0624027370272 docs/s, 0.00161653747819617 MB/s).
Processed 111000 documents (1704.5427802788238 docs/s, 0.001625578670767616 MB/s).
Processed 112000 documents (1713.5042752850509 docs/s, 0.0016341250183916577 MB/s).
Processed 113000 documents (1722.1557919990416 docs/s, 0.00164237574767975 MB/s).
Processed 114000 documents (1731.1781395285234 docs/s, 0.0016509801287923082 MB/s).
Processed 115000 documents (1740.341690932212 docs/s, 0.0016597191724130745 MB/s).
Processed 116000 documents (1749.8564073658642 docs/s, 0.0016687931131037371 MB/s).
Processed 117000 documents (1760.044217377728 docs/s, 0.001678508965852478 MB/s).
Processed 118000 documents (1768.6566942298082 docs/s, 0.0016867224638269502 MB/s).
Processed 119000 documents (1777.4506328166026 docs/s, 0.00169510901719723 MB/s).
Processed 120000 documents (1787.4395680627688 docs/s, 0.0017046352081897438 MB/s).
Processed 121000 documents (1796.492544432945 docs/s, 0.0017132687992410134 MB/s).
Processed 122000 documents (1805.623860059099 docs/s, 0.0017219771004286755 MB/s).
Processed 123000 documents (1813.355241061874 docs/s, 0.0017293503199213734 MB/s).
Processed 124000 documents (1821.7785380916266 docs/s, 0.0017373834019581095 MB/s).
Processed 125000 documents (1829.9844967480853 docs/s, 0.0017452092139702657 MB/s).
Processed 126000 documents (1838.8589601430833 docs/s, 0.0017536725617819627 MB/s).
Processed 127000 documents (1847.4559939252897 docs/s, 0.0017618713320973298 MB/s).
Processed 128000 documents (1855.9119005748682 docs/s, 0.0017699355130909617 MB/s).
Processed 129000 documents (1863.5308969314437 docs/s, 0.0017772015542330205 MB/s).
Processed 130000 documents (1871.5284290090071 docs/s, 0.0017848285951700279 MB/s).
Processed 131000 documents (1880.8374517921302 docs/s, 0.001793706371109133 MB/s).
Processed 132000 documents (1887.3767578268917 docs/s, 0.0017999427393216054 MB/s).
Processed 133000 documents (1894.920582965713 docs/s, 0.0018071370916039591 MB/s).
Processed 134000 documents (1902.6536237791377 docs/s, 0.0018145118940154435 MB/s).
Processed 135000 documents (1908.6476972923952 docs/s, 0.0018202282879756881 MB/s).
Processed 136000 documents (1917.2212894818788 docs/s, 0.00182840470264614 MB/s).
Processed 137000 documents (1926.1148436438732 docs/s, 0.0018368862568320019 MB/s).
Processed 138000 documents (1933.6305206372115 docs/s, 0.001844053764950954 MB/s).
Processed 139000 documents (1942.160507505538 docs/s, 0.0018521885943465594 MB/s).
Processed 140000 documents (1950.506323756749 docs/s, 0.0018601477849547853 MB/s).
Processed 141000 documents (1958.930833730518 docs/s, 0.0018681820237450772 MB/s).
Processed 142000 documents (1967.26030639622 docs/s, 0.001876125627895565 MB/s).
Processed 143000 documents (1974.7841351113261 docs/s, 0.0018833009101022016 MB/s).
Processed 144000 documents (1982.2987268115714 docs/s, 0.0018904673832050051 MB/s).
Processed 145000 documents (1989.9952959898628 docs/s, 0.0018978074035547856 MB/s).
Processed 146000 documents (1997.8301087667164 docs/s, 0.0019052792632739224 MB/s).
Processed 147000 documents (2005.7146809330472 docs/s, 0.0019127985772448037 MB/s).
Processed 148000 documents (2012.6808586968955 docs/s, 0.001919442042061706 MB/s).
Processed 149000 documents (2019.5509447061315 docs/s, 0.0019259938666402164 MB/s).
Processed 150000 documents (2027.112368884358 docs/s, 0.0019332050026744441 MB/s).
Processed 151000 documents (2034.520172283094 docs/s, 0.0019402696345168057 MB/s).
Processed 152000 documents (2041.3415213578985 docs/s, 0.0019467749799326882 MB/s).
Processed 153000 documents (2049.0355167809957 docs/s, 0.0019541125457582433 MB/s).
Processed 154000 documents (2057.074655650096 docs/s, 0.0019617792660237272 MB/s).
Processed 155000 documents (2065.304032465534 docs/s, 0.0019696274113326397 MB/s).
Processed 156000 documents (2072.820167248482 docs/s, 0.001976795356033785 MB/s).
Processed 157000 documents (2080.297344785587 docs/s, 0.0019839261482101317 MB/s).
Processed 158000 documents (2086.0183786048196 docs/s, 0.0019893821512268254 MB/s).
Processed 159000 documents (2093.384401799497 docs/s, 0.001996406938361642 MB/s).
Processed 160000 documents (2100.9132518641045 docs/s, 0.0020035870093003316 MB/s).
Processed 161000 documents (2108.7028455060886 docs/s, 0.0020110157446919332 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/visual-basic
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/visual-basic/gpt2-preprocessed
Time to startup: 0.8376128673553467
END TIME: Wed Mar 29 01:07:51 UTC 2023
