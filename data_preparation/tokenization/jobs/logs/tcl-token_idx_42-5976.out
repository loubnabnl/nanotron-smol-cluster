START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/tcl
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/tcl --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/tcl/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (435.0669227366153 docs/s, 0.0004149121501318124 MB/s).
Processed 2000 documents (689.415623714836 docs/s, 0.0006574779736660347 MB/s).
Processed 3000 documents (837.7671787248926 docs/s, 0.0007989570414780546 MB/s).
Processed 4000 documents (999.3986529468597 docs/s, 0.0009531008271664235 MB/s).
Processed 5000 documents (1082.7303499330144 docs/s, 0.0010325721263246674 MB/s).
Processed 6000 documents (1134.6985310903306 docs/s, 0.001082132845964747 MB/s).
Processed 7000 documents (1165.3897580628068 docs/s, 0.0011114022808673923 MB/s).
Processed 8000 documents (1246.9999583025588 docs/s, 0.001189231832792815 MB/s).
Processed 9000 documents (1302.9891122380973 docs/s, 0.001242627250898454 MB/s).
Processed 10000 documents (1357.121511230677 docs/s, 0.0012942519295031328 MB/s).
Processed 11000 documents (1382.7708048330232 docs/s, 0.0013187130020456536 MB/s).
Processed 12000 documents (1407.9014222667495 docs/s, 0.0013426794264476294 MB/s).
Processed 13000 documents (1420.0653752566295 docs/s, 0.001354279876000051 MB/s).
Processed 14000 documents (1365.6761495468404 docs/s, 0.0013024102683514027 MB/s).
Processed 15000 documents (1392.5571918041765 docs/s, 0.0013280460279504551 MB/s).
Processed 16000 documents (1399.8538044784357 docs/s, 0.0013350046200546605 MB/s).
Processed 17000 documents (1414.3378766935448 docs/s, 0.0013488177077231834 MB/s).
Processed 18000 documents (1456.6337602403576 docs/s, 0.0013891542055514885 MB/s).
Processed 19000 documents (1498.5247597914959 docs/s, 0.0014291045759119948 MB/s).
Processed 20000 documents (1525.2407225611482 docs/s, 0.0014545829034434779 MB/s).
Processed 21000 documents (1560.6234727377914 docs/s, 0.0014883265235307612 MB/s).
Processed 22000 documents (1608.0884184461845 docs/s, 0.0015335926231824727 MB/s).
Processed 23000 documents (1655.4128817384326 docs/s, 0.001578724748362 MB/s).
Processed 24000 documents (1704.619876625868 docs/s, 0.001625652195573681 MB/s).
Processed 25000 documents (1748.4153185363477 docs/s, 0.0016674187836993673 MB/s).
Processed 26000 documents (1787.2694487249296 docs/s, 0.0017044729697465225 MB/s).
Processed 27000 documents (1830.9958938239047 docs/s, 0.001746173757385163 MB/s).
Processed 28000 documents (1874.0554085427766 docs/s, 0.001787238510649468 MB/s).
Processed 29000 documents (1916.3595355885054 docs/s, 0.0018275828700909666 MB/s).
Processed 30000 documents (1950.8270925964548 docs/s, 0.0018604536939587162 MB/s).
Processed 31000 documents (1981.6602644661887 docs/s, 0.001889858498064221 MB/s).
Processed 32000 documents (2013.933740428326 docs/s, 0.001920636883190466 MB/s).
Processed 33000 documents (2052.975330305992 docs/s, 0.001957869844728462 MB/s).
Processed 34000 documents (2088.0690837271763 docs/s, 0.0019913378560325397 MB/s).
Processed 35000 documents (2119.5595597538054 docs/s, 0.0020213695142305426 MB/s).
Processed 36000 documents (2147.245569045955 docs/s, 0.0020477729502162505 MB/s).
Processed 37000 documents (2182.325203608736 docs/s, 0.0020812274967276917 MB/s).
Processed 38000 documents (2207.372223480751 docs/s, 0.0021051141962821493 MB/s).
Processed 39000 documents (2240.0774722556716 docs/s, 0.002136304352050468 MB/s).
Processed 40000 documents (2276.1775156281174 docs/s, 0.0021707320362359213 MB/s).
Processed 41000 documents (2304.1704158051252 docs/s, 0.0021974281461764576 MB/s).
Processed 42000 documents (2336.438641629993 docs/s, 0.002228201524381631 MB/s).
Processed 43000 documents (2356.5986912575495 docs/s, 0.002247427645928907 MB/s).
Processed 44000 documents (2376.7729040739455 docs/s, 0.0022666672745456176 MB/s).
Processed 45000 documents (2403.076907210943 docs/s, 0.0022917527267560414 MB/s).
Processed 46000 documents (2433.1174989863375 docs/s, 0.00232040166758188 MB/s).
Processed 47000 documents (2461.9489554847255 docs/s, 0.002347897487148977 MB/s).
Processed 48000 documents (2490.7459236681148 docs/s, 0.002375360416095843 MB/s).
Processed 49000 documents (2513.765084955704 docs/s, 0.00239731319900103 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/tcl
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/tcl/gpt2-preprocessed
Time to startup: 0.808842658996582
END TIME: Wed Mar 29 01:06:53 UTC 2023
