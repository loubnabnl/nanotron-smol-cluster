START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/fortran
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/fortran --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/fortran/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (353.38631440912934 docs/s, 0.00033701545182145056 MB/s).
Processed 2000 documents (397.5357790627091 docs/s, 0.0003791196623446551 MB/s).
Processed 3000 documents (505.0658665406194 docs/s, 0.0004816683450132555 MB/s).
Processed 4000 documents (576.0892030213922 docs/s, 0.0005494014768804476 MB/s).
Processed 5000 documents (636.5727488904039 docs/s, 0.0006070830811409034 MB/s).
Processed 6000 documents (668.4701799147239 docs/s, 0.0006375028418681372 MB/s).
Processed 7000 documents (713.5801264718042 docs/s, 0.0006805230393140833 MB/s).
Processed 8000 documents (757.5863322113054 docs/s, 0.0007224906274903349 MB/s).
Processed 9000 documents (793.3725553428989 docs/s, 0.0007566190293721188 MB/s).
Processed 10000 documents (824.9621998404559 docs/s, 0.000786745261993843 MB/s).
Processed 11000 documents (857.6187941973749 docs/s, 0.0008178890172933339 MB/s).
Processed 12000 documents (893.3351094139937 docs/s, 0.000851950749792093 MB/s).
Processed 13000 documents (930.8411365565601 docs/s, 0.0008877192845883942 MB/s).
Processed 14000 documents (942.7496271162124 docs/s, 0.000899076106182301 MB/s).
Processed 15000 documents (957.0714804364794 docs/s, 0.0009127344898571772 MB/s).
Processed 16000 documents (973.5026543881886 docs/s, 0.0009284044784433256 MB/s).
Processed 17000 documents (969.3601718614178 docs/s, 0.0009244538992513827 MB/s).
Processed 18000 documents (975.866340457017 docs/s, 0.0009306586651392145 MB/s).
Processed 19000 documents (983.6524530799967 docs/s, 0.0009380840807723967 MB/s).
Processed 20000 documents (959.2555655146135 docs/s, 0.0009148173957010398 MB/s).
Processed 21000 documents (962.1579614635788 docs/s, 0.0009175853361736095 MB/s).
Processed 22000 documents (975.1082527048263 docs/s, 0.0009299356963203681 MB/s).
Processed 23000 documents (978.2119392790421 docs/s, 0.0009328956024923726 MB/s).
Processed 24000 documents (989.4494000536424 docs/s, 0.0009436124802147316 MB/s).
Processed 25000 documents (1003.1049804001179 docs/s, 0.0009566354564667873 MB/s).
Processed 26000 documents (1011.3340795652198 docs/s, 0.000964483336987705 MB/s).
Processed 27000 documents (1027.3290749533094 docs/s, 0.0009797373532803625 MB/s).
Processed 28000 documents (1042.1746099507257 docs/s, 0.0009938951587207085 MB/s).
Processed 29000 documents (1055.0326909271614 docs/s, 0.001006157580306207 MB/s).
Processed 30000 documents (1062.4798234658388 docs/s, 0.001013259719339217 MB/s).
Processed 31000 documents (1073.2601791759116 docs/s, 0.001023540667701637 MB/s).
Processed 32000 documents (1086.0488250244903 docs/s, 0.0010357368707890418 MB/s).
Processed 33000 documents (1086.9426974751173 docs/s, 0.0010365893339873478 MB/s).
Processed 34000 documents (1097.5879759453578 docs/s, 0.0010467414626554087 MB/s).
Processed 35000 documents (1100.0388668713888 docs/s, 0.0010490788143838776 MB/s).
Processed 36000 documents (1097.0080283697228 docs/s, 0.0010461883815476635 MB/s).
Processed 37000 documents (1106.7755294802635 docs/s, 0.0010555033964922556 MB/s).
Processed 38000 documents (1115.8031703785489 docs/s, 0.001064112825754689 MB/s).
Processed 39000 documents (1119.5983990751358 docs/s, 0.001067732237887512 MB/s).
Processed 40000 documents (1122.8328524094068 docs/s, 0.0010708168529600208 MB/s).
Processed 41000 documents (1133.3055808239355 docs/s, 0.0010808044250716548 MB/s).
Processed 42000 documents (1140.4703930424773 docs/s, 0.0010876373224663518 MB/s).
Processed 43000 documents (1143.6126079798175 docs/s, 0.0010906339721487213 MB/s).
Processed 44000 documents (1149.3955149420178 docs/s, 0.0010961489819927386 MB/s).
Processed 45000 documents (1153.3721778054332 docs/s, 0.0010999414232305844 MB/s).
Processed 46000 documents (1151.4476881596424 docs/s, 0.001098106086883204 MB/s).
Processed 47000 documents (1158.5272078364403 docs/s, 0.0011048576429714587 MB/s).
Processed 48000 documents (1158.0166559814963 docs/s, 0.001104370742780205 MB/s).
Processed 49000 documents (1163.5363869848084 docs/s, 0.001109634768471535 MB/s).
Processed 50000 documents (1162.6994378442425 docs/s, 0.0011088365915720392 MB/s).
Processed 51000 documents (1129.65880575781 docs/s, 0.00107732658935338 MB/s).
Processed 52000 documents (1136.294481375078 docs/s, 0.001083654862761572 MB/s).
Processed 53000 documents (1146.1806574663585 docs/s, 0.0010930830549872957 MB/s).
Processed 54000 documents (1151.551984848668 docs/s, 0.001098205551956814 MB/s).
Processed 55000 documents (1149.8020135792208 docs/s, 0.0010965366493026932 MB/s).
Processed 56000 documents (1152.4380319358675 docs/s, 0.0010990505523070025 MB/s).
Processed 57000 documents (1157.974615739432 docs/s, 0.0011043306500810928 MB/s).
Processed 58000 documents (1162.659441990872 docs/s, 0.001108798448553917 MB/s).
Processed 59000 documents (1166.673647299111 docs/s, 0.0011126266930571662 MB/s).
Processed 60000 documents (1172.9223678428468 docs/s, 0.001118585937350127 MB/s).
Processed 61000 documents (1170.4285594208434 docs/s, 0.0011162076563080248 MB/s).
Processed 62000 documents (1174.6959042241317 docs/s, 0.0011202773134461705 MB/s).
Processed 63000 documents (1179.1848488295318 docs/s, 0.001124558304624111 MB/s).
Processed 64000 documents (1177.396227415466 docs/s, 0.0011228525423197422 MB/s).
Processed 65000 documents (1177.9227249002886 docs/s, 0.00112335464944867 MB/s).
Processed 66000 documents (1182.755372210988 docs/s, 0.0011279634210691338 MB/s).
Processed 67000 documents (1187.2822564740013 docs/s, 0.0011322805943241132 MB/s).
Processed 68000 documents (1193.116043033265 docs/s, 0.001137844126733079 MB/s).
Processed 69000 documents (1199.5318755249664 docs/s, 0.0011439627413987793 MB/s).
Processed 70000 documents (1202.3351293982357 docs/s, 0.0011466361326200826 MB/s).
Processed 71000 documents (1202.5982985937794 docs/s, 0.001146887110322742 MB/s).
Processed 72000 documents (1204.9909411373778 docs/s, 0.0011491689120649126 MB/s).
Processed 73000 documents (1202.5290672604833 docs/s, 0.0011468210861782869 MB/s).
Processed 74000 documents (1205.7547888291945 docs/s, 0.0011498973739902444 MB/s).
Processed 75000 documents (1211.1435379048612 docs/s, 0.0011550364855812656 MB/s).
Processed 76000 documents (1214.6735961166246 docs/s, 0.0011584030114332434 MB/s).
Processed 77000 documents (1214.8532297734403 docs/s, 0.0011585743234381106 MB/s).
Processed 78000 documents (1218.2244536853038 docs/s, 0.0011617893730977095 MB/s).
Processed 79000 documents (1219.100520931359 docs/s, 0.001162624855929717 MB/s).
Processed 80000 documents (1222.3358368807596 docs/s, 0.00116571029365612 MB/s).
Processed 81000 documents (1222.3626346256253 docs/s, 0.001165735849977136 MB/s).
Processed 82000 documents (1223.7993786284767 docs/s, 0.001167106035831906 MB/s).
Processed 83000 documents (1228.9511564230033 docs/s, 0.0011720191539983781 MB/s).
Processed 84000 documents (1182.4615537720067 docs/s, 0.0011276832139701907 MB/s).
Processed 85000 documents (1186.1001269756093 docs/s, 0.0011311532277828306 MB/s).
Processed 86000 documents (1191.4921003933514 docs/s, 0.0011362954143460764 MB/s).
Processed 87000 documents (1195.3829167908157 docs/s, 0.0011400059860141904 MB/s).
Processed 88000 documents (1198.6669258861668 docs/s, 0.0011431378611432713 MB/s).
Processed 89000 documents (1197.681972239346 docs/s, 0.0011421985361474475 MB/s).
Processed 90000 documents (1199.7621550003366 docs/s, 0.0011441823530200354 MB/s).
Processed 91000 documents (1202.0966176849915 docs/s, 0.0011464086701249996 MB/s).
Processed 92000 documents (1205.7762359837868 docs/s, 0.001149917827590739 MB/s).
Processed 93000 documents (1209.256346647631 docs/s, 0.0011532367197490988 MB/s).
Processed 94000 documents (1214.7005878061589 docs/s, 0.0011584287527143086 MB/s).
Processed 95000 documents (1214.161202362177 docs/s, 0.0011579143546697397 MB/s).
Processed 96000 documents (1221.4764124228238 docs/s, 0.0011648906826236952 MB/s).
Processed 97000 documents (1228.557542694601 docs/s, 0.0011716437746950158 MB/s).
Processed 98000 documents (1234.9157741782244 docs/s, 0.0011777074567587132 MB/s).
Processed 99000 documents (1242.116613411458 docs/s, 0.0011845747121920186 MB/s).
Processed 100000 documents (1249.5898311614867 docs/s, 0.0011917017280211322 MB/s).
Processed 101000 documents (1256.4421724269748 docs/s, 0.001198236629893279 MB/s).
Processed 102000 documents (1262.1551059580665 docs/s, 0.0012036849078732171 MB/s).
Processed 103000 documents (1267.9344669715215 docs/s, 0.0012091965360369887 MB/s).
Processed 104000 documents (1274.9901928016911 docs/s, 0.0012159254005448257 MB/s).
Processed 105000 documents (1281.6643003077463 docs/s, 0.0012222903254582847 MB/s).
Processed 106000 documents (1288.7965000697423 docs/s, 0.001229092121190779 MB/s).
Processed 107000 documents (1295.1694631104517 docs/s, 0.0012351698523621099 MB/s).
Processed 108000 documents (1300.2004640424454 docs/s, 0.0012399677887367682 MB/s).
Processed 109000 documents (1307.3339675431582 docs/s, 0.001246770827811392 MB/s).
Processed 110000 documents (1313.4977566962505 docs/s, 0.0012526490752184395 MB/s).
Processed 111000 documents (1319.398821205514 docs/s, 0.0012582767688803806 MB/s).
Processed 112000 documents (1325.8843247345817 docs/s, 0.0012644618270250146 MB/s).
Processed 113000 documents (1331.5011089784905 docs/s, 0.0012698184098992257 MB/s).
Processed 114000 documents (1338.1601776122748 docs/s, 0.0012761689926264522 MB/s).
Processed 115000 documents (1344.3444474168884 docs/s, 0.001282066771904839 MB/s).
Processed 116000 documents (1350.3890573235692 docs/s, 0.0012878313611255352 MB/s).
Processed 117000 documents (1356.5953928909075 docs/s, 0.0012937501839551044 MB/s).
Processed 118000 documents (1362.625377773769 docs/s, 0.0012995008256662074 MB/s).
Processed 119000 documents (1368.5824850530273 docs/s, 0.0013051819658785127 MB/s).
Processed 120000 documents (1373.8062415894312 docs/s, 0.0013101637283224404 MB/s).
Processed 121000 documents (1380.088321709085 docs/s, 0.0013161547867861605 MB/s).
Processed 122000 documents (1386.0258707431344 docs/s, 0.0013218172748023361 MB/s).
Processed 123000 documents (1391.1249360288057 docs/s, 0.0013266801224029595 MB/s).
Processed 124000 documents (1396.030457249725 docs/s, 0.0013313583919999361 MB/s).
Processed 125000 documents (1400.3202696300268 docs/s, 0.0013354494758892315 MB/s).
Processed 126000 documents (1405.7613828928986 docs/s, 0.0013406385258606897 MB/s).
Processed 127000 documents (1407.5341357673087 docs/s, 0.00134232915474635 MB/s).
Processed 128000 documents (1411.866884713082 docs/s, 0.0013464611861353703 MB/s).
Processed 129000 documents (1416.5052280964028 docs/s, 0.0013508846550907162 MB/s).
Processed 130000 documents (1421.576652379521 docs/s, 0.001355721142177125 MB/s).
Processed 131000 documents (1427.5521074522283 docs/s, 0.0013614197802088053 MB/s).
Processed 132000 documents (1432.9181988038545 docs/s, 0.001366537283710341 MB/s).
Processed 133000 documents (1439.2152474170089 docs/s, 0.001372542617241868 MB/s).
Processed 134000 documents (1444.2412226073004 docs/s, 0.001377335760695744 MB/s).
Processed 135000 documents (1449.8357879565733 docs/s, 0.0013826711539808019 MB/s).
Processed 136000 documents (1454.2389117106384 docs/s, 0.001386870300017012 MB/s).
Processed 137000 documents (1460.2565456467958 docs/s, 0.00139260916294746 MB/s).
Processed 138000 documents (1464.9721120755923 docs/s, 0.0013971062775379108 MB/s).
Processed 139000 documents (1470.2482594026524 docs/s, 0.0014021380037333034 MB/s).
Processed 140000 documents (1475.1763397056236 docs/s, 0.0014068377873474346 MB/s).
Processed 141000 documents (1479.54395670224 docs/s, 0.001411003071501007 MB/s).
Processed 142000 documents (1484.1691340866796 docs/s, 0.0014154139843813702 MB/s).
Processed 143000 documents (1488.4712860618295 docs/s, 0.001419516836225347 MB/s).
Processed 144000 documents (1492.6454291680823 docs/s, 0.0014234976092987845 MB/s).
Processed 145000 documents (1497.5344833706408 docs/s, 0.0014281601747232827 MB/s).
Processed 146000 documents (1500.679837672952 docs/s, 0.0014311598183373948 MB/s).
Processed 147000 documents (1505.8880169927986 docs/s, 0.0014361267251899706 MB/s).
Processed 148000 documents (1510.0717011187073 docs/s, 0.0014401165972888063 MB/s).
Processed 149000 documents (1515.258661875923 docs/s, 0.00144506326854317 MB/s).
Processed 150000 documents (1520.3640247916644 docs/s, 0.0014499321220318455 MB/s).
Processed 151000 documents (1524.7121954229603 docs/s, 0.0014540788606862644 MB/s).
Processed 152000 documents (1528.7900001367805 docs/s, 0.001457967758309155 MB/s).
Processed 153000 documents (1532.7122179788232 docs/s, 0.0014617082767284615 MB/s).
Processed 154000 documents (1535.9684482942357 docs/s, 0.0014648136599485738 MB/s).
Processed 155000 documents (1539.0359488852866 docs/s, 0.00146773905647782 MB/s).
Processed 156000 documents (1543.673366702362 docs/s, 0.0014721616427444096 MB/s).
Processed 157000 documents (1549.357340400418 docs/s, 0.0014775823024753742 MB/s).
Processed 158000 documents (1554.321800470184 docs/s, 0.0014823167805387344 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/fortran
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/fortran/gpt2-preprocessed
Time to startup: 0.8489890098571777
END TIME: Wed Mar 29 01:08:16 UTC 2023
