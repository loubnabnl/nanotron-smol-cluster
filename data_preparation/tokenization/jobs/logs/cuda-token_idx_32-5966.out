START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/cuda
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/cuda --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/cuda/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (450.90879380426986 docs/s, 0.0004300201356928538 MB/s).
Processed 2000 documents (578.8472073509096 docs/s, 0.0005520317147740455 MB/s).
Processed 3000 documents (728.8225833132924 docs/s, 0.0006950593789227413 MB/s).
Processed 4000 documents (854.1066898240279 docs/s, 0.0008145396135559348 MB/s).
Processed 5000 documents (921.2408060505627 docs/s, 0.0008785636959558131 MB/s).
Processed 6000 documents (1007.5635860306013 docs/s, 0.0009608875141435636 MB/s).
Processed 7000 documents (1055.7564636483885 docs/s, 0.0010068478237613568 MB/s).
Processed 8000 documents (1112.1249021426208 docs/s, 0.0010606049558092316 MB/s).
Processed 9000 documents (1138.19991650911 docs/s, 0.0010854720273104762 MB/s).
Processed 10000 documents (1181.7895318900148 docs/s, 0.001127042323961272 MB/s).
Processed 11000 documents (1190.5581190183848 docs/s, 0.001135404700296769 MB/s).
Processed 12000 documents (1221.2496597573488 docs/s, 0.001164674434430455 MB/s).
Processed 13000 documents (1239.4581576801932 docs/s, 0.0011820394112398082 MB/s).
Processed 14000 documents (1277.1540260397783 docs/s, 0.0012179889927289756 MB/s).
Processed 15000 documents (1309.1801041770473 docs/s, 0.0012485314409037087 MB/s).
Processed 16000 documents (1342.1919367359492 docs/s, 0.001280013977752637 MB/s).
Processed 17000 documents (1377.7214828478204 docs/s, 0.0013138975933531002 MB/s).
Processed 18000 documents (1403.9923813661098 docs/s, 0.0013389514745389079 MB/s).
Processed 19000 documents (1397.8546732929126 docs/s, 0.0013330980999879003 MB/s).
Processed 20000 documents (1416.8696132918815 docs/s, 0.0013512321598929229 MB/s).
Processed 21000 documents (1434.4175292744565 docs/s, 0.0013679671566719594 MB/s).
Processed 22000 documents (1336.9453219317265 docs/s, 0.0012750104159657731 MB/s).
Processed 23000 documents (1341.8471715939045 docs/s, 0.001279685184091477 MB/s).
Processed 24000 documents (1334.431616165024 docs/s, 0.0012726131593370667 MB/s).
Processed 25000 documents (1340.956421780884 docs/s, 0.0012788356988724557 MB/s).
Processed 26000 documents (1352.9768998084935 docs/s, 0.0012902993200383124 MB/s).
Processed 27000 documents (1378.7298554583747 docs/s, 0.0013148592524131533 MB/s).
Processed 28000 documents (1387.1763452879482 docs/s, 0.0013229144528274042 MB/s).
Processed 29000 documents (1407.2236851346963 docs/s, 0.0013420330859515155 MB/s).
Processed 30000 documents (1430.1308619238378 docs/s, 0.001363879072116697 MB/s).
Processed 31000 documents (1442.2792461282222 docs/s, 0.001375464674118254 MB/s).
Processed 32000 documents (1469.9688129589526 docs/s, 0.001401871502837136 MB/s).
Processed 33000 documents (1495.7322170089392 docs/s, 0.0014264413995828049 MB/s).
Processed 34000 documents (1520.375652139945 docs/s, 0.0014499432107352686 MB/s).
Processed 35000 documents (1546.3818246572878 docs/s, 0.0014747446295330885 MB/s).
Processed 36000 documents (1571.8849449125407 docs/s, 0.0014990663003087432 MB/s).
Processed 37000 documents (1596.9031268840092 docs/s, 0.0015229254978981106 MB/s).
Processed 38000 documents (1620.6227447289423 docs/s, 0.0015455462882317946 MB/s).
Processed 39000 documents (1642.144777609493 docs/s, 0.0015660712982268267 MB/s).
Processed 40000 documents (1667.3170746056796 docs/s, 0.00159007747135704 MB/s).
Processed 41000 documents (1688.6563168769571 docs/s, 0.001610428158642728 MB/s).
Processed 42000 documents (1709.851882688094 docs/s, 0.0016306418253785077 MB/s).
Processed 43000 documents (1731.6618185979953 docs/s, 0.0016514414010982469 MB/s).
Processed 44000 documents (1751.116612226255 docs/s, 0.0016699949381125021 MB/s).
Processed 45000 documents (1770.3041148525854 docs/s, 0.0016882935665632108 MB/s).
Processed 46000 documents (1788.629988466674 docs/s, 0.001705770481554674 MB/s).
Processed 47000 documents (1809.3287717814364 docs/s, 0.0017255103795828213 MB/s).
Processed 48000 documents (1827.049773233902 docs/s, 0.0017424104435290356 MB/s).
Processed 49000 documents (1846.1152844744179 docs/s, 0.0017605927319282702 MB/s).
Processed 50000 documents (1863.1257275558328 docs/s, 0.001776815154605706 MB/s).
Processed 51000 documents (1875.1397548204598 docs/s, 0.0017882726238445852 MB/s).
Processed 52000 documents (1890.3972529826542 docs/s, 0.0018028233079744856 MB/s).
Processed 53000 documents (1906.20607831941 docs/s, 0.001817899778670702 MB/s).
Processed 54000 documents (1923.8002805224064 docs/s, 0.001834678917429358 MB/s).
Processed 55000 documents (1939.5470351299598 docs/s, 0.0018496961928653334 MB/s).
Processed 56000 documents (1955.2181901359656 docs/s, 0.0018646413709029824 MB/s).
Processed 57000 documents (1968.6680051312521 docs/s, 0.0018774681140244027 MB/s).
Processed 58000 documents (1985.4986312673063 docs/s, 0.0018935190498993934 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/cuda
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/cuda/gpt2-preprocessed
Time to startup: 0.7890253067016602
END TIME: Wed Mar 29 01:07:04 UTC 2023
