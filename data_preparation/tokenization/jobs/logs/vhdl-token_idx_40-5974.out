START TIME: Wed Mar 29 01:06:11 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/vhdl
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/vhdl --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/vhdl/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (207.88713409200525 docs/s, 0.00019825662049484754 MB/s).
Processed 2000 documents (347.1413689107859 docs/s, 0.0003310598076923236 MB/s).
Processed 3000 documents (446.75434233132626 docs/s, 0.00042605814202435136 MB/s).
Processed 4000 documents (486.769990054666 docs/s, 0.0004642200375124607 MB/s).
Processed 5000 documents (547.3043222868429 docs/s, 0.0005219500754230908 MB/s).
Processed 6000 documents (591.5378005428995 docs/s, 0.0005641344075612063 MB/s).
Processed 7000 documents (629.1185523959509 docs/s, 0.0005999742053946981 MB/s).
Processed 8000 documents (664.6496208174598 docs/s, 0.0006338592727827642 MB/s).
Processed 9000 documents (698.6132571439992 docs/s, 0.0006662495204391472 MB/s).
Processed 10000 documents (730.6136131216916 docs/s, 0.0006967674380509297 MB/s).
Processed 11000 documents (743.850853800177 docs/s, 0.0007093914545060892 MB/s).
Processed 12000 documents (728.7768783420757 docs/s, 0.0006950157912655599 MB/s).
Processed 13000 documents (750.1555315950226 docs/s, 0.0007154040637922502 MB/s).
Processed 14000 documents (762.1891748140098 docs/s, 0.0007268802402629946 MB/s).
Processed 15000 documents (759.792974500341 docs/s, 0.000724595045566884 MB/s).
Processed 16000 documents (762.7472094072612 docs/s, 0.0007274124235222446 MB/s).
Processed 17000 documents (763.6388186995935 docs/s, 0.0007282627284046111 MB/s).
Processed 18000 documents (768.6232006042904 docs/s, 0.0007330162054102807 MB/s).
Processed 19000 documents (768.7688948431805 docs/s, 0.0007331551502639585 MB/s).
Processed 20000 documents (781.8743202028886 docs/s, 0.0007456534578350912 MB/s).
Processed 21000 documents (785.0135296153308 docs/s, 0.000748647241225558 MB/s).
Processed 22000 documents (787.0470593894797 docs/s, 0.0007505865663428113 MB/s).
Processed 23000 documents (797.0177493419495 docs/s, 0.0007600953572673316 MB/s).
Processed 24000 documents (800.458496774805 docs/s, 0.0007633767097232866 MB/s).
Processed 25000 documents (811.7076592803346 docs/s, 0.0007741047470858904 MB/s).
Processed 26000 documents (814.3467940827944 docs/s, 0.0007766216221645302 MB/s).
Processed 27000 documents (823.9977003837055 docs/s, 0.0007858254436337523 MB/s).
Processed 28000 documents (833.8822726674932 docs/s, 0.0007952521063494618 MB/s).
Processed 29000 documents (837.7940051148939 docs/s, 0.0007989826251172008 MB/s).
Processed 30000 documents (802.1298363092274 docs/s, 0.0007649706233112596 MB/s).
Processed 31000 documents (808.7055777197716 docs/s, 0.0007712417390058246 MB/s).
Processed 32000 documents (809.3093984257755 docs/s, 0.0007718175873048549 MB/s).
Processed 33000 documents (765.9011281589534 docs/s, 0.0007304202348317655 MB/s).
Processed 34000 documents (772.7736476776169 docs/s, 0.0007369743801857155 MB/s).
Processed 35000 documents (781.7568816565403 docs/s, 0.0007455414597096827 MB/s).
Processed 36000 documents (789.9838290127278 docs/s, 0.0007533872881057051 MB/s).
Processed 37000 documents (800.2439180445601 docs/s, 0.000763172071499405 MB/s).
Processed 38000 documents (809.5516514061522 docs/s, 0.000772048617750313 MB/s).
Processed 39000 documents (813.1413436288444 docs/s, 0.0007754720150268978 MB/s).
Processed 40000 documents (819.2889656606803 docs/s, 0.000781334844265633 MB/s).
Processed 41000 documents (827.6061954592597 docs/s, 0.0007892667727081868 MB/s).
Processed 42000 documents (837.7518399643778 docs/s, 0.0007989424132961062 MB/s).
Processed 43000 documents (847.9447413032136 docs/s, 0.0008086631215126167 MB/s).
Processed 44000 documents (857.3547008248216 docs/s, 0.0008176371582267968 MB/s).
Processed 45000 documents (867.9509475180198 docs/s, 0.0008277425265484045 MB/s).
Processed 46000 documents (877.3093395315835 docs/s, 0.0008366673846546016 MB/s).
Processed 47000 documents (888.4892739207771 docs/s, 0.0008473294009406825 MB/s).
Processed 48000 documents (898.0505362774599 docs/s, 0.0008564477312826727 MB/s).
Processed 49000 documents (907.5266600028453 docs/s, 0.0008654848670986608 MB/s).
Processed 50000 documents (916.8007130050466 docs/s, 0.0008743292932558505 MB/s).
Processed 51000 documents (923.2931088930386 docs/s, 0.0008805209244661699 MB/s).
Processed 52000 documents (930.5328761978044 docs/s, 0.0008874253046014828 MB/s).
Processed 53000 documents (940.2107146417932 docs/s, 0.0008966548105638439 MB/s).
Processed 54000 documents (949.5160699531115 docs/s, 0.0009055290889292826 MB/s).
Processed 55000 documents (957.4657206709417 docs/s, 0.0009131104666432778 MB/s).
Processed 56000 documents (965.8894192617241 docs/s, 0.0009211439316384546 MB/s).
Processed 57000 documents (973.0945416535014 docs/s, 0.0009280152718100561 MB/s).
Processed 58000 documents (981.5650095056042 docs/s, 0.0009360933394485513 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/vhdl
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/vhdl/gpt2-preprocessed
Time to startup: 0.8280127048492432
END TIME: Wed Mar 29 01:07:32 UTC 2023
