START TIME: Wed Mar 29 01:06:14 UTC 2023
FULL_OUT_PATH is: /fsx/loubna/data/tokenized_stack_no_pii/code/r
/fsx/loubna/code/Megatron-LM/tools/preprocess_data.py --input /fsx/loubna/data/stack_march_no_pii_json/r --output-prefix /fsx/loubna/data/tokenized_stack_no_pii/code/r/gpt2-preprocessed --tokenizer-type TokenizerFromFile --tokenizer-file /fsx/loubna/data/tokenizer/tokenizer-the-stack-march-sample-v3-no-prefix-spaces/tokenizer.json --dataset-impl mmap --append-eod --json-keys content --workers 64 --chunk-size 100 --log-interval 1000
[H[2J[3J> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
> building TokenizerFromFile tokenizer ...
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
 > padded vocab (size: 49152) with 0 dummy tokens (new size: 49152)
Processed 1000 documents (300.8925689082997 docs/s, 0.0002869535149653432 MB/s).
Processed 2000 documents (498.4364975953727 docs/s, 0.00047534608611619254 MB/s).
Processed 3000 documents (616.6799089838192 docs/s, 0.0005881117906416123 MB/s).
Processed 4000 documents (704.5234932274944 docs/s, 0.0006718859607958741 MB/s).
Processed 5000 documents (799.5932550027671 docs/s, 0.0007625515508678123 MB/s).
Processed 6000 documents (848.2857352134716 docs/s, 0.0008089883186468808 MB/s).
Processed 7000 documents (748.4392077040774 docs/s, 0.0007137672497788213 MB/s).
Processed 8000 documents (809.0608634806337 docs/s, 0.0007715805659109437 MB/s).
Processed 9000 documents (875.0887549369698 docs/s, 0.0008345496701593112 MB/s).
Processed 10000 documents (938.2128916945053 docs/s, 0.0008947495381302884 MB/s).
Processed 11000 documents (991.796101466625 docs/s, 0.0009458504690805674 MB/s).
Processed 12000 documents (1044.275651112974 docs/s, 0.000995898867714857 MB/s).
Processed 13000 documents (1107.9253898986033 docs/s, 0.0010565999888406785 MB/s).
Processed 14000 documents (1170.0281616500831 docs/s, 0.0011158258072377044 MB/s).
Processed 15000 documents (1226.1967268708815 docs/s, 0.0011693923252781692 MB/s).
Processed 16000 documents (1280.8929809445667 docs/s, 0.0012215547379918735 MB/s).
Processed 17000 documents (1333.0850004163601 docs/s, 0.0012713289264834977 MB/s).
Processed 18000 documents (1389.5919099409746 docs/s, 0.0013252181147966142 MB/s).
Processed 19000 documents (1430.9784138601526 docs/s, 0.001364687360630181 MB/s).
Processed 20000 documents (1478.469662866375 docs/s, 0.0014099785450614691 MB/s).
Processed 21000 documents (1525.0591685056293 docs/s, 0.00145440976000369 MB/s).
Processed 22000 documents (1572.858066035478 docs/s, 0.001499994340930441 MB/s).
Processed 23000 documents (1617.865310492175 docs/s, 0.0015429165940210104 MB/s).
Processed 24000 documents (1664.6222436653325 docs/s, 0.001587507480302174 MB/s).
Processed 25000 documents (1702.9193571437852 docs/s, 0.00162403045381907 MB/s).
Processed 26000 documents (1741.3106761127037 docs/s, 0.0016606432686926877 MB/s).
Processed 27000 documents (1773.9851290423544 docs/s, 0.0016918040552543205 MB/s).
Processed 28000 documents (1815.3968352997676 docs/s, 0.0017312973359105755 MB/s).
Processed 29000 documents (1851.7431889323639 docs/s, 0.0017659599198650015 MB/s).
Processed 30000 documents (1889.4711274314159 docs/s, 0.001801940085822502 MB/s).
Processed 31000 documents (1920.520916871713 docs/s, 0.0018315514725415355 MB/s).
Processed 32000 documents (1958.771925241537 docs/s, 0.001868030476800477 MB/s).
Processed 33000 documents (1993.5508475037657 docs/s, 0.001901198241714254 MB/s).
Processed 34000 documents (2019.2028705502378 docs/s, 0.0019256619172575357 MB/s).
Processed 35000 documents (2045.9375932941036 docs/s, 0.0019511581356946026 MB/s).
Processed 36000 documents (2080.1513276727997 docs/s, 0.0019837868954399106 MB/s).
Processed 37000 documents (2105.196010854665 docs/s, 0.002007671366552987 MB/s).
Processed 38000 documents (2138.425038233659 docs/s, 0.002039361036523494 MB/s).
Processed 39000 documents (2169.545016520417 docs/s, 0.002069039360542695 MB/s).
Opening /fsx/loubna/data/stack_march_no_pii_json/r
Input is not a jsonl file, will try to load from HF datasets
Vocab size: 49152
Output prefix: /fsx/loubna/data/tokenized_stack_no_pii/code/r/gpt2-preprocessed
Time to startup: 0.8212189674377441
END TIME: Wed Mar 29 01:06:55 UTC 2023
